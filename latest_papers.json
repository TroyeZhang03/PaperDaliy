[
  {
    "title": "Spatia: Video Generation with Updatable Spatial Memory",
    "url": "http://arxiv.org/abs/2512.15716v1",
    "abstract_en": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
    "date": "2025-12-17",
    "summary_cn": "针对现有视频生成模型在长序列生成中难以维持时空一致性的挑战，本文提出了Spatia框架。其核心创新在于显式地构建并维护一个3D场景点云作为空间记忆。该框架以迭代方式生成视频片段，并利用视觉SLAM技术动态更新空间记忆。这种将动态内容与静态场景解耦的设计，不仅显著提升了生成视频的空间连贯性，还支持精确的相机控制和3D感知的交互式编辑等高级应用，为基于记忆的可扩展视频生成提供了几何基础。",
    "one_sentence": "本文提出了一种名为Spatia的视频生成框架，通过引入并持续更新一个3D场景点云作为持久性空间记忆，来解决生成长视频时难以保持时空一致性的问题。"
  },
  {
    "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
    "url": "http://arxiv.org/abs/2512.15715v1",
    "abstract_en": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.",
    "date": "2025-12-17",
    "summary_cn": "论文指出，作为视觉信息源头的像素包含从底层属性到高层概念的丰富信息。作者提出了一种名为Pixio的增强型掩码自编码器模型，通过在20亿网络图像上进行自监督预训练，证明了这种基于像素的自学习方法在简单性、稳定性和效率上具有优势。实验表明，Pixio在单目深度估计、3D重建、语义分割和机器人学习等多种下游任务中表现出色，性能与同规模训练的DINOv3相当或更优，表明像素空间的自监督学习是潜在空间方法的有力替代和补充。",
    "one_sentence": "本文提出了一种名为Pixio的增强型掩码自编码器，通过在20亿网络图像上进行具有挑战性的预训练任务，证明了基于像素的自监督学习依然是学习通用视觉表征的有效方法。"
  },
  {
    "title": "Large Isolated Stripes on Short 18-leg $t$-$J$ Cylinders",
    "url": "http://arxiv.org/abs/2512.15714v1",
    "abstract_en": "Spin-charge stripes belong to the most prominent low-temperature orders besides superconductivity in high-temperature superconductors. This phase is particularly challenging to study numerically due to finite-size effects. By investigating the formation of long, isolated stripes, we offer a perspective complementary to typical finite-doping phase diagrams. We use the density-matrix renormalization group algorithm to extract the ground states of an 18-leg cylindrical strip geometry, making the diameter significantly wider than in previous works. This approach allows us to map out the range of possible stripe filling fractions on the electron versus hole-doped side. We find good agreement with established results, suggesting that the spread of filling fractions observed in the literature is governed by the physics of a single stripe. Taking a microscopic look at stripe formation, we reveal two separate regimes - a high-filling regime captured by a simplified squeezed-space model and a low-filling regime characterized by the structure of individual pairs of dopants. Thereby, we trace back the phenomenology of the striped phase to its microscopic constituents and highlight the different challenges for observing the two regimes in quantum simulation experiments.",
    "date": "2025-12-17",
    "summary_cn": "条纹相是高温超导体中除超导外最重要的低温有序相。针对数值模拟中的尺寸效应难题，本研究采用密度矩阵重正化群方法，在比以往工作更宽的18腿圆柱几何系统中研究了孤立长条纹的形成。结果表明，文献中观察到的条纹填充分数范围受单一条纹的物理机制支配。微观上，条纹形成存在两种机制：高填充区符合简化的挤压空间模型，而低填充区则由单个掺杂剂对的结构决定。这为理解条纹相的本质及其在量子模拟实验中的观测挑战提供了新视角。",
    "one_sentence": "本文通过研究更宽圆柱几何系统中的孤立条纹，揭示了条纹填充分数由单一条纹物理主导，并识别出其形成的高填充与低填充两种微观机制。"
  },
  {
    "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
    "url": "http://arxiv.org/abs/2512.15713v1",
    "abstract_en": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
    "date": "2025-12-17",
    "summary_cn": "针对扩散视觉语言模型性能落后的问题，本研究提出了DiffusionVL，它能将现有的强大自回归模型转化为扩散范式。该方法不仅实现了范式转换的有效性，还通过引入块解码设计，在仅使用少量数据微调后，便在多个基准测试中获得超过34%的性能提升，同时推理速度提升一倍。",
    "one_sentence": "本文提出了一种名为DiffusionVL的新方法，可将任何强大的自回归预训练模型通过简单微调转换为性能优异的扩散视觉语言模型。"
  },
  {
    "title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
    "url": "http://arxiv.org/abs/2512.15712v1",
    "abstract_en": "Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.",
    "date": "2025-12-17",
    "summary_cn": "论文提出了一种名为预测概念解码器（PCD）的可扩展神经网络解释方法。该方法通过通信瓶颈将解释任务转化为端到端的训练目标：编码器将复杂的内部激活压缩成稀疏的概念列表，解码器则根据此列表回答关于模型行为的自然语言问题。研究表明，PCD可以通过预训练和微调来提升解释能力，并在检测越狱攻击、秘密提示和潜在概念等下游任务中表现优异。",
    "one_sentence": "本文提出了一种名为预测概念解码器（PCD）的新方法，通过端到端训练助手从模型内部激活中提取稀疏概念来预测和解释模型行为。"
  }
]