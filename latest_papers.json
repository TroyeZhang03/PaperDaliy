[
  {
    "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
    "url": "http://arxiv.org/abs/2602.06043v1",
    "abstract_en": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.",
    "date": "2026-02-05",
    "summary_cn": "该论文针对大型预训练模型持续适应新任务时存在的灾难性遗忘和重训练成本高的问题，提出了一种名为Share的创新方法。该方法通过学习一个动态更新的共享低秩子空间，在持续学习过程中有效整合新知识并促进前向知识迁移，同时显著减少了参数量和内存消耗。实验在多个任务和模态上验证了其有效性，为大规模AI系统的终身学习提供了实用且可扩展的解决方案。",
    "one_sentence": "本文提出了一种名为Share的参数高效持续微调方法，通过学习并动态更新一个共享的低秩子空间，实现了跨任务和模态的持续学习，无需数据回放或多个适配器。"
  },
  {
    "title": "Pseudo-Invertible Neural Networks",
    "url": "http://arxiv.org/abs/2602.06042v1",
    "abstract_en": "The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or \"Back-Projection\", $x' = x + A^\\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, \"degradation\" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.",
    "date": "2026-02-05",
    "summary_cn": "该研究将摩尔-彭罗斯伪逆从线性系统推广到非线性领域，提出了名为SPNN的神经网络架构。该架构具有可处理的非线性伪逆，并满足关键几何特性，如非线性回投影。这解决了传统方法难以处理的非线性信息损失问题，将零样本反问题的求解范围扩展到了光学畸变、分类等多种非线性退化场景，从而在不重新训练扩散先验模型的情况下，实现了对复杂退化过程的零样本反演和对生成输出的精确语义控制。",
    "one_sentence": "本文提出了一种非线性伪逆（PInv）及其实现的网络（SPNN），将零样本反问题的求解范围从线性映射扩展到了非线性映射（如分类）。"
  },
  {
    "title": "Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning",
    "url": "http://arxiv.org/abs/2602.06041v1",
    "abstract_en": "Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.",
    "date": "2026-02-05",
    "summary_cn": "针对当前多模态大语言模型在跨多张图片的空间推理任务（尤其是视角转换推理）上的不足，本文提出了CAMCUE框架。该框架将相机位姿信息显式注入视觉表征，并以此为基础，将自然语言描述的目标视点精准地映射到具体位姿，从而合成条件化目标视角来支持问答。为了支持这一研究方向，作者构建了CAMCUE-DATA数据集用于训练和测试。实验表明，CAMCUE不仅显著提升了推理准确率，还能高效地从语言描述中预测目标位姿，极大地提升了推理速度，使其适用于需要快速交互的真实场景。",
    "one_sentence": "本文提出了一种名为CAMCUE的新型多视角空间推理框架，通过显式利用相机位姿作为几何锚点，有效实现了跨视图融合和指定新视点的空间推理。"
  },
  {
    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
    "url": "http://arxiv.org/abs/2602.06039v1",
    "abstract_en": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.",
    "date": "2026-02-05",
    "summary_cn": "现有基于大语言模型的多智能体系统通常采用固定的通信模式，难以适应迭代问题解决中不同阶段的需求。为此，本文提出了DyTopo框架。在每个推理回合，由管理员设定目标，各智能体生成轻量级的需求与能力描述符。DyTopo通过语义匹配这些描述符，动态构建稀疏有向通信图，仅沿图中边路由私有信息。在代码生成和数学推理等任务上的实验表明，DyTopo性能优于最强基线，并能通过演化的通信图提供可解释的协作轨迹。",
    "one_sentence": "本文提出了一种名为DyTopo的管理员引导多智能体框架，它能根据每轮任务目标动态重构稀疏有向通信图，以实现更匹配问题解决阶段需求的通信路由。"
  },
  {
    "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
    "url": "http://arxiv.org/abs/2602.06040v1",
    "abstract_en": "Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as \"visual thoughts\" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.",
    "date": "2026-02-05",
    "summary_cn": "现有MLLM主要依赖文本思维链，在视觉密集型任务上受限，而引入固定视觉推理模式又会损害文本逻辑。本文提出SwimBird模型，其核心创新是能够根据用户查询自适应地选择最合适的推理模式，包括纯文本、纯视觉和视觉文本交织推理。通过统一的混合自回归框架和构建多样化的微调数据集，该模型在保持强大文本逻辑推理能力的同时，显著提升了在视觉密集型任务上的性能，在多个基准测试中取得了优异结果。",
    "one_sentence": "本文提出了SwimBird，一种能够根据输入动态在纯文本、纯视觉和视觉文本交织三种推理模式间切换的多模态大语言模型。"
  }
]