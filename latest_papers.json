[
  {
    "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
    "url": "http://arxiv.org/abs/2511.21692v1",
    "abstract_en": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
    "date": "2025-11-26",
    "summary_cn": "本研究探讨了大型语言模型在不同难度任务上的泛化能力。与以往依赖人类判断不同，作者使用数千个不同LLM的输出和项目反应理论，为六个数据集中的样本提供了客观、细粒度的难度评级。通过系统评估发现，模型在难度维度上的泛化能力有限，仅使用简单或困难数据训练无法在所有难度级别上取得一致改进。这强调了在模型训练和评估数据中包含全范围难度的重要性。",
    "one_sentence": "本文创新地利用大规模语言模型自身输出和项目反应理论来客观量化任务难度，系统评估了模型在不同难度数据上的泛化能力。"
  },
  {
    "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls",
    "url": "http://arxiv.org/abs/2511.21691v1",
    "abstract_en": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.",
    "date": "2025-11-26",
    "summary_cn": "现有的扩散模型在同时处理文本、主体参考、空间布局等多种控制信号时存在困难。为此，本研究提出了Canvas-to-Image框架，其核心是将不同控制信号整合到一个画布图像中供模型直接解读。通过构建多任务数据集和联合训练策略，模型学会了跨模态推理，而非依赖特定任务的启发式方法。实验表明，该方法在身份保持和控制遵循方面显著优于现有技术，尤其在多人合成、姿态控制等复杂场景下表现优异。",
    "one_sentence": "本文提出了一种名为Canvas-to-Image的统一框架，通过将多种异构控制信号编码到一个复合画布中，并采用多任务画布训练策略，使扩散模型能够联合理解并忠实生成符合复杂约束的图像。"
  },
  {
    "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos",
    "url": "http://arxiv.org/abs/2511.21690v1",
    "abstract_en": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.",
    "date": "2025-11-26",
    "summary_cn": "针对机器人从少量演示中学习新任务的挑战，本文引入了TraceGen世界模型。该模型的核心创新在于使用一种名为“trace-space”的符号化3D轨迹空间来表示运动，从而能够从人类或其他机器人的异构视频中学习，并忽略外观差异。研究者构建了包含大量视频数据的数据管道TraceForge用于预训练，使模型获得了一个可迁移的3D运动先验知识。实验表明，仅需五个目标演示视频，该模型就能在多种任务上实现高成功率，并显著加快推理速度，尤其在仅有人类手持手机拍摄的演示视频时，仍能有效适应并在真实机器人上完成任务。",
    "one_sentence": "本文提出了一种名为TraceGen的世界模型，通过统一的3D轨迹空间表示法，利用跨平台、跨环境的视频数据高效学习机器人任务。"
  },
  {
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "url": "http://arxiv.org/abs/2511.21689v1",
    "abstract_en": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
    "date": "2025-11-26",
    "summary_cn": "该论文针对大型语言模型解决复杂问题时成本高、效率低的问题，提出了ToolOrchestra方法。该方法通过强化学习训练小型编排模型来管理和协调多种工具，在性能、成本和用户偏好之间取得了最佳平衡。实验表明，其研发的Orchestrator模型在多项基准测试中超越了GPT-5等强大模型，且效率大幅提升，证明了轻量级编排模型结合多样化工具是一种更高效、更有效的解决方案。",
    "one_sentence": "本文提出了一种名为ToolOrchestra的方法，通过强化学习训练小型编排模型来协调多种智能工具，在保证高性能的同时显著提升了解决复杂任务的效率和用户偏好对齐。"
  },
  {
    "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
    "url": "http://arxiv.org/abs/2511.21688v1",
    "abstract_en": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
    "date": "2025-11-26",
    "summary_cn": "当前视觉语言模型在空间智能方面存在不足，主要由于缺乏从2D图像重建3D空间的几何学习能力。为此，本文提出了G²VLM模型，它利用学习到的3D视觉几何特征，不仅能直接预测3D属性，还能通过上下文学习和交错推理增强空间理解任务。该模型设计统一且可扩展，能够利用丰富的多视角图像和视频数据进行训练，同时受益于通常需要难以获取的标注数据才能得到的3D视觉先验。实验表明，G²VLM在3D重建和空间理解任务上均表现出色，有望成为该领域的强基准并推动如3D场景编辑等应用的发展。",
    "one_sentence": "本文提出了一种名为G²VLM的几何基础视觉语言模型，通过将3D视觉几何特征学习与语义理解相结合，统一了空间3D重建和空间理解两大任务。"
  }
]