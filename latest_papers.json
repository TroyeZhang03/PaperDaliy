[
  {
    "title": "A self-consistent explanation of the MeV line in GRB 221009A unveils a dense circum-stellar medium",
    "url": "http://arxiv.org/abs/2601.14257v1",
    "abstract_en": "GRB~221009A has been the brightest gamma-ray burst (GRB) observed to date, and its afterglow has been characterised with unprecedented detail at TeV energies by LHAASO. Quite puzzlingly, it is also the most energetic GRB known. Among the riddles posed by this mysterious source, however, the sheer energetics are hardly the most intriguing: an unprecedented, narrow, luminous emission line at around 10 MeV has been uncovered by a detailed spectral analysis of \\textit{Fermi}/GBM data immediately following the brightest peak in the GRB prompt emission and the peak of the TeV afterglow. As noted in the discovery article, the temporal evolution of the line properties can be explained as being due to high-latitude emission from a geometrically thin, relativistically expanding shell where annihilation of a large number of electron-positron pairs took place. We show that this interpretation yields stringent constraints on the properties of such shell, that point to a process that happens at radii typical of external shocks. We then demonstrate that the shell could have been the blastwave associated with the GRB precursor, with the line arising after pair loading of such blastwave as it was illuminated by the bright and hard radiation of the GRB main event. The scenario, which also explains the abrupt initial rise of the LHAASO afterglow, requires the progenitor of the GRB to have been surrounded by a circum-stellar medium (CSM) extending out to a few $10^{15}\\,\\mathrm{cm}$, with a density $n_\\mathrm{ext}\\sim 10^{8}-10^{9}\\,\\mathrm{cm^{-3}}$ reminiscent of those found from studies of Type IIn supernovae. This provides a precious clue to the nature of the progenitor of this peculiar GRB, which could also be present in other bursts that feature a long quiescence followed by a bright emission episode with a hard spectrum.",
    "date": "2026-01-20",
    "summary_cn": "针对史上最亮的伽马射线暴GRB 221009A，研究者在费米卫星数据中发现了一个前所未有的10 MeV窄发射线。分析表明，该线可解释为相对论性膨胀薄壳中电子-正电子对湮灭产生的高纬度辐射，且此薄壳可能是伽马暴前兆的激波。该模型要求前身星周围存在一个延伸至约10^15 cm、密度高达10^8-10^9 cm^-3的环星介质，类似于IIn型超新星的环境，这为理解此类伽马暴的前身星性质提供了关键线索。",
    "one_sentence": "本文通过分析GRB 221009A中发现的10 MeV窄发射线，提出该线源于伽马暴前兆激波在硬辐射照射下产生电子-正电子对湮灭的高纬度辐射，并据此推断其前身星周围存在一个延展且高密度的环星介质。"
  },
  {
    "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
    "url": "http://arxiv.org/abs/2601.14256v1",
    "abstract_en": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.",
    "date": "2026-01-20",
    "summary_cn": "传统的图像表示学习模型通常专注于识别或生成单一任务。本文提出了一种创新的统一模型，它通过超网络学习隐式神经表示来快速、准确地重建图像，并结合知识蒸馏提升泛化性能。该模型学习到一个高度压缩的嵌入空间，在多种视觉任务上取得了与最先进方法相竞争的性能，同时保留了高质量的图像生成能力。",
    "one_sentence": "本文提出了一种首个将图像识别与生成能力统一的学习模型，通过超网络学习隐式神经表示并集成知识蒸馏，实现了高性能的压缩嵌入空间。"
  },
  {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "url": "http://arxiv.org/abs/2601.14255v1",
    "abstract_en": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
    "date": "2026-01-20",
    "summary_cn": "针对视频抠图领域缺乏标注数据的问题，本文提出了VideoMaMa模型，它能够利用预训练视频扩散模型将粗糙的分割掩码转化为精准的alpha遮罩，在仅使用合成数据训练的情况下，对真实视频展现了优秀的零样本泛化能力。基于此，作者构建了一个大规模伪标注视频抠图数据集MA-V，包含超过5万个高质量标注视频。在该数据集上微调的SAM2-Matte模型，在真实视频上的鲁棒性超越了使用现有数据集训练的模型。",
    "one_sentence": "本文提出了VideoMaMa模型，利用预训练视频扩散模型将粗糙分割掩码转换为高质量遮罩，并构建了大规模伪标注视频数据集MA-V，显著提升了视频抠图在真实场景下的零样本泛化能力。"
  },
  {
    "title": "Using observations of escaping H/He to constrain the atmospheric composition of sub-Neptunes",
    "url": "http://arxiv.org/abs/2601.14254v1",
    "abstract_en": "The internal composition of sub-Neptunes remains a prominent unresolved question in exoplanetary science. We present a technique to place constraints on envelope mean molecular weight that utilises observations of escaping hydrogen or helium exospheres. This method is based on a simple timescale argument, which states that sub-Neptunes require a sufficiently large hydrogen or helium reservoir to explain on-going escape at their observed rates. This then naturally leads to an upper limit on atmospheric mean molecular weight. We apply this technique to archetypal sub-Neptunes, namely GJ-436 b, TOI-776 b and TOI-776 c, which have all been observed to be losing significant hydrogen content as well as relatively featureless transit spectra when observed with JWST. Combining constraints from atmospheric escape and transit spectroscopy in the case of TOI-776 c allows us to tentatively rule out the high mean molecular weight scenario, pointing towards a low mean molecular weight atmosphere with high-altitude aerosols muting spectral features in the infra-red. Finally, we reframe our analysis to the hycean candidate K2-18 b, which has also been shown to host a tentative escaping hydrogen exosphere. If such a detection is robust, we infer a hydrogen-rich envelope mass fraction of $\\log f_\\text{env} = -1.67\\pm0.78$, which is inconsistent with the hycean scenario at the $\\sim 4σ$ level. This latter result requires further observational follow-up to confirm.",
    "date": "2026-01-20",
    "summary_cn": "亚海王星内部结构是系外行星科学的重要未解问题。本研究提出一种新方法，利用观测到的氢或氦大气逃逸现象，通过时标论证来约束其大气平均分子量的上限。将此方法应用于GJ-436 b等典型亚海王星，并结合凌星光谱数据，初步排除了TOI-776 c高平均分子量大气的可能性。对Hycean候选体K2-18 b的分析则表明，若其氢逃逸观测属实，其富氢包层质量分数与Hycean模型存在显著矛盾，有待后续观测证实。",
    "one_sentence": "本文提出了一种利用系外行星氢或氦逃逸观测数据，来约束其大气平均分子量和内部组成的创新方法。"
  },
  {
    "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
    "url": "http://arxiv.org/abs/2601.14253v1",
    "abstract_en": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.",
    "date": "2026-01-20",
    "summary_cn": "本文提出了Motion 3-to-4框架，旨在解决从单目视频合成高质量4D动态对象的难题。该方法将问题分解为静态3D形状生成和运动重建两部分，利用参考网格学习紧凑运动表示并预测顶点轨迹，从而恢复出完整、时序一致的几何体。其可扩展的逐帧Transformer结构能适应不同长度的序列。实验表明，该方法在保真度和空间一致性上优于现有技术。",
    "one_sentence": "本文提出了一种名为Motion 3-to-4的前馈框架，通过将4D动态对象合成分解为静态3D形状生成和运动重建，并使用紧凑运动潜在表示与逐帧顶点轨迹预测，实现了从单目视频生成高质量4D内容。"
  }
]