[
  {
    "title": "MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues",
    "url": "http://arxiv.org/abs/2512.03046v1",
    "abstract_en": "We propose MagicQuill V2, a novel system that introduces a \\textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.",
    "date": "2025-12-02",
    "summary_cn": "MagicQuill V2系统创新性地采用分层组合方法进行生成式图像编辑。它将用户的创意意图解构为内容、空间、结构和色彩四个独立的视觉提示层，从而克服了单一提示无法区分不同编辑意图的局限。该系统包含一个用于上下文感知内容生成的数据管道、一个统一处理所有视觉提示的控制模块，以及一个用于精确局部编辑的微调空间分支。实验证明，该方法能有效解决用户意图差距，提供直观精细的生成控制。",
    "one_sentence": "本文提出了MagicQuill V2系统，通过一种分层组合范式将创意意图解构为可控的视觉提示层，以弥合扩散模型语义能力与传统图形软件精细控制之间的差距。"
  },
  {
    "title": "CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models",
    "url": "http://arxiv.org/abs/2512.03045v1",
    "abstract_en": "Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear. In this work, we first verify that the attention maps of these models acquire geometric correspondence throughout training, attending to the geometrically corresponding regions across reference and target views for view-consistent generation. However, this correspondence signal remains incomplete, with its accuracy degrading under large viewpoint changes. Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models. Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance. CAMEO reduces the number of training iterations required for convergence by half while achieving superior performance at the same iteration counts. We further demonstrate that CAMEO is model-agnostic and can be applied to any multi-view diffusion model.",
    "date": "2025-12-02",
    "summary_cn": "本文研究发现，多视图扩散模型的注意力图在训练中能学习到几何对应关系，但这种关系并不完善。基于此，作者提出了CAMEO训练技术，通过直接利用几何对应关系监督注意力图，有效提升了模型的学习效率和生成质量。该方法仅需监督单个注意力层，即可加速模型收敛并改善新视图合成效果，且适用于多种模型。实验表明，CAMEO能将训练迭代次数减半，并在相同迭代次数下获得更优性能。",
    "one_sentence": "本文提出了一种名为CAMEO的训练技术，通过直接监督多视图扩散模型的注意力图以学习几何对应关系，从而提升训练效率和生成质量。"
  },
  {
    "title": "Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling",
    "url": "http://arxiv.org/abs/2512.03044v1",
    "abstract_en": "Robust perception and dynamics modeling are fundamental to real-world robotic policy learning. Recent methods employ video diffusion models (VDMs) to enhance robotic policies, improving their understanding and modeling of the physical world. However, existing approaches overlook the coherent and physically consistent motion representations inherently encoded across frames in VDMs. To this end, we propose Video2Act, a framework that efficiently guides robotic action learning by explicitly integrating spatial and motion-aware representations. Building on the inherent representations of VDMs, we extract foreground boundaries and inter-frame motion variations while filtering out background noise and task-irrelevant biases. These refined representations are then used as additional conditioning inputs to a diffusion transformer (DiT) action head, enabling it to reason about what to manipulate and how to move. To mitigate inference inefficiency, we propose an asynchronous dual-system design, where the VDM functions as the slow System 2 and the DiT head as the fast System 1, working collaboratively to generate adaptive actions. By providing motion-aware conditions to System 1, Video2Act maintains stable manipulation even with low-frequency updates from the VDM. For evaluation, Video2Act surpasses previous state-of-the-art VLA methods by 7.7% in simulation and 21.7% in real-world tasks in terms of average success rate, further exhibiting strong generalization capabilities.",
    "date": "2025-12-02",
    "summary_cn": "针对现有方法忽略视频扩散模型中连贯运动表征的问题，本文提出Video2Act框架。该框架从VDM中提取前景边界和帧间运动变化等关键信息，作为扩散Transformer动作头的条件输入，使其能推理操作对象和移动方式。为提升效率，采用异步双系统设计：VDM作为慢速系统提供稳定的运动感知条件，DiT作为快速系统生成自适应动作。实验表明，Video2Act在仿真和真实任务中的平均成功率均显著超越现有最优方法，并展现出强大的泛化能力。",
    "one_sentence": "本文提出了Video2Act框架，通过从视频扩散模型中提取并利用连贯的物理运动表征，并结合异步双系统设计，来高效指导机器人动作学习。"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "url": "http://arxiv.org/abs/2512.03043v1",
    "abstract_en": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.",
    "date": "2025-12-02",
    "summary_cn": "针对现有方法需为不同任务分别训练模型、割裂图像与视频推理的问题，本文提出了OneThinker一体化推理模型。该模型通过构建覆盖问答、描述、定位、跟踪、分割等任务的OneThinker-600k训练语料，并利用EMA-GRPO算法解决多任务强化学习中的奖励异质性问题，实现了对图像和视频的统一理解。实验表明，OneThinker在31个基准测试的10项任务上表现优异，并展现出知识迁移和零样本泛化能力，推动了通用多模态推理模型的发展。",
    "one_sentence": "本文提出了一种名为OneThinker的统一多模态推理模型，通过构建大规模训练语料和EMA-GRPO优化方法，实现了图像与视频跨多种基础视觉任务的全能理解。"
  },
  {
    "title": "PPTArena: A Benchmark for Agentic PowerPoint Editing",
    "url": "http://arxiv.org/abs/2512.03042v1",
    "abstract_en": "We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.",
    "date": "2025-12-02",
    "summary_cn": "该研究介绍了首个PowerPoint编辑基准PPTArena，它包含大量真实幻灯片和针对性的编辑指令，并采用双重视觉语言模型进行自动化评估。基于此，作者提出了PPTPilot智能体，它通过规划语义编辑序列、在程序工具与XML操作间路由以及迭代验证来精确编辑幻灯片。实验表明，PPTPilot在复杂和布局敏感的任务上优于现有前沿系统，但文档级的长期编辑任务仍面临挑战。",
    "one_sentence": "本文提出了PPT编辑基准PPTArena和结构感知的编辑智能体PPTPilot，通过结合高级程序工具与确定性XML操作，在复杂编辑任务上显著优于现有系统。"
  }
]