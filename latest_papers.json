[
  {
    "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
    "url": "http://arxiv.org/abs/2511.21692v1",
    "abstract_en": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
    "date": "2025-11-26",
    "summary_cn": "本研究探讨了大语言模型在不同难度任务间的泛化能力。作者利用数千个LLM的输出和项目反应理论，对六个数据集中的样本进行了客观、细粒度的难度评级，避免了人为判断。研究发现，模型在不同难度间的泛化能力有限，仅使用简单或困难数据进行训练无法在所有难度上获得一致提升，强调了训练和评估数据中难度多样性的重要性。",
    "one_sentence": "本文通过结合数千种大语言模型和项目反应理论来客观量化任务难度，系统评估了模型在不同难度数据上的泛化能力。"
  },
  {
    "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls",
    "url": "http://arxiv.org/abs/2511.21691v1",
    "abstract_en": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.",
    "date": "2025-11-26",
    "summary_cn": "针对现有扩散模型难以同时精确控制文本、参考主体、空间布局等多种模态的问题，本文提出了Canvas-to-Image框架。其核心是将不同控制信号统一编码为一张画布图像，并设计多任务数据集和联合训练策略，使模型能直接对画布进行视觉-空间推理。实验表明，该方法在身份保持和控制遵循方面显著优于现有技术，尤其在多人合成、姿态控制等复杂场景下表现出色。",
    "one_sentence": "本文提出了一个名为Canvas-to-Image的统一框架，通过将多种异构控制信号编码到一张复合画布中，并采用多任务画布训练策略，使扩散模型能够联合理解并生成高保真度的多模态控制图像。"
  },
  {
    "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos",
    "url": "http://arxiv.org/abs/2511.21690v1",
    "abstract_en": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.",
    "date": "2025-11-26",
    "summary_cn": "针对机器人从少量演示中学习新任务的挑战，本文引入了一种统一的符号表示——3D轨迹空间，它能从人类或不同机器人的视频中抽象出几何运动信息。提出的TraceGen模型在此空间预测未来运动，避免了外观差异。通过TraceForge数据管道构建大规模训练集进行预训练后，模型仅需5个目标视频即可高效适应新任务，在真实机器人上取得了高成功率，并显著提升了推理速度。",
    "one_sentence": "本文提出了一种名为TraceGen的世界模型，通过一种统一的3D轨迹空间符号表示法，利用跨形态、跨环境的视频数据来高效学习机器人任务。"
  },
  {
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "url": "http://arxiv.org/abs/2511.21689v1",
    "abstract_en": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
    "date": "2025-11-26",
    "summary_cn": "论文针对解决复杂任务时大模型成本高、效率低的问题，提出了ToolOrchestra方法。该方法训练一个小型编排模型，利用强化学习综合考虑结果、效率和用户偏好来协调多种工具。由此产生的8B参数Orchestrator模型，在Humanity's Last Exam等基准测试中，其性能超越了GPT-5，同时成本显著降低，实现了性能与成本的最佳平衡，并展现出对未见过工具的鲁棒泛化能力。",
    "one_sentence": "本文提出了一种名为ToolOrchestra的方法，通过强化学习训练小型编排模型来协调多种智能工具，以更低的成本实现优于大型语言模型的性能。"
  },
  {
    "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
    "url": "http://arxiv.org/abs/2511.21688v1",
    "abstract_en": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
    "date": "2025-11-26",
    "summary_cn": "针对当前视觉语言模型在空间智能方面表现不佳的问题，本文提出了一种几何接地的模型G²VLM。该模型的核心创新在于将空间3D重建和空间理解这两个关键方面统一起来，能够利用从丰富的多视角图像和视频数据中学习到的3D几何特征，直接预测3D属性并通过情境学习等方式提升空间推理任务的表现。实验表明，G²VLM在3D重建和空间理解任务上均取得了优异成果，为未来应用（如3D场景编辑）提供了有力基础。",
    "one_sentence": "本文提出了G²VLM模型，通过将3D几何重建与空间理解相结合，利用从多视角图像学习到的3D视觉特征来增强视觉语言模型的空间推理能力。"
  }
]