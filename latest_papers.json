[
  {
    "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
    "url": "http://arxiv.org/abs/2601.16214v1",
    "abstract_en": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.",
    "date": "2026-01-22",
    "summary_cn": "针对现有视频扩散模型相机控制能力有限的问题，本文基于奖励反馈学习框架，提出了一种高效的相机感知3D解码器。该方法将视频潜变量与相机姿态解码为3D高斯表示，相机姿态作为投影参数，若与视频内容不匹配会导致3D结构畸变和渲染模糊。通过优化渲染新视图与真实视图的像素一致性作为奖励，并引入可见性项选择性地监督确定性区域，从而有效提升了相机控制的对齐质量。在多个基准测试上的实验验证了该方法的有效性。",
    "one_sentence": "本文提出了一种高效的相机感知3D解码器，通过将视频潜变量解码为3D高斯表示并优化渲染视图的一致性，以提升视频扩散模型的相机控制能力。"
  },
  {
    "title": "Gauge Theory and Skein Modules",
    "url": "http://arxiv.org/abs/2601.16213v1",
    "abstract_en": "We study skein modules of 3-manifolds by embedding them into the Hilbert spaces of 4d ${\\cal N}=4$ super-Yang-Mills theories. When the 3-manifold has reduced holonomy, we present an algorithm to determine the dimension and the list of generators of the skein module with a general gauge group. The analysis uses a deformation preserving ${\\cal N}=1$ supersymmetry to express the dimension as a sum over nilpotent orbits. We find that the dimensions often differ between Langlands-dual pairs beyond the A-series, for which we provide a physical explanation involving chiral symmetry breaking and 't Hooft operators. We also relate our results to the structure of $\\mathbb{C}^*$-fixed loci in the moduli space of Higgs bundles. This approach helps to clarify the relation between the gauge-theoretic framework of Kapustin and Witten with other versions of the geometric Langlands program, explains why the dimensions of skein modules do not exhibit a TQFT-like behavior, and provides a physical interpretation of the skein-valued curve counting of Ekholm and Shende.",
    "date": "2026-01-22",
    "summary_cn": "本研究将三维流形的skein模嵌入4d N=4超对称杨-米尔斯理论的希尔伯特空间进行分析。对于约化完整群的流形，作者提出了一种算法，利用保持N=1超对称性的形变，将skein模的维度表达为幂零轨道的求和，从而确定其维度和生成元。研究发现，在A系列之外，Langlands对偶群的skein模维度常不相同，这源于手征对称性破缺和't Hooft算子的物理机制。该工作还联系了Higgs丛模空间的C*不动点轨迹，有助于澄清Kapustin-Witten规范理论框架与几何Langlands纲领其他版本的关系，解释了skein模维度非拓扑量子场论行为的原因，并为Ekholm-Shende的skein值曲线计数提供了物理解释。",
    "one_sentence": "本文通过将三维流形的skein模嵌入4d超对称杨-米尔斯理论的希尔伯特空间，提出了一种计算其维数与生成元的算法，并解释了Langlands对偶群下维数差异的物理原因。"
  },
  {
    "title": "Point Bridge: 3D Representations for Cross Domain Policy Learning",
    "url": "http://arxiv.org/abs/2601.16212v1",
    "abstract_en": "Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/",
    "date": "2026-01-22",
    "summary_cn": "为解决机器人基础模型因缺乏大规模真实数据而受限的问题，本文提出了Point Bridge框架。该框架利用基于点的统一表征来弥合仿真与现实的视觉差距，通过视觉语言模型自动提取点表征，结合Transformer进行策略学习，仅使用合成数据就能训练出可在现实世界零样本执行的机器人操作策略。实验表明，该方法在零样本迁移和结合少量真实数据时性能显著优于现有方法。",
    "one_sentence": "本文提出了Point Bridge框架，利用基于点的统一表征，结合视觉语言模型和Transformer策略学习，实现了无需视觉对齐的零样本仿真到现实策略迁移。"
  },
  {
    "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition",
    "url": "http://arxiv.org/abs/2601.16211v1",
    "abstract_en": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.",
    "date": "2026-01-22",
    "summary_cn": "本文研究组合视频理解（CVU），发现现有零样本组合动作识别（ZS-CAR）模型因对象驱动的动词捷径而失效，根源在于监督数据的稀疏性、偏态分布以及动词与对象学习难度不对称。为此，作者提出了RCORE框架，它通过组合感知的数据增强来丰富动词-对象组合，并引入时间顺序正则化损失来惩罚模型对共现统计的依赖，从而强制模型进行基于时间的动词学习。在Sth-com和新构建的EK100-com基准测试上，RCORE显著提升了未见组合的识别准确率，降低了共现偏见，并获得了稳定的正向组合性能增益。",
    "one_sentence": "本文提出了RCORE框架，通过组合感知数据增强和时间顺序正则化损失，解决了零样本组合动作识别中因对象驱动的动词捷径而导致模型失效的问题。"
  },
  {
    "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
    "url": "http://arxiv.org/abs/2601.16210v1",
    "abstract_en": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
    "date": "2026-01-22",
    "summary_cn": "现有离散视频VAE的分词器通常存在词汇量有限、语言监督不足等问题，导致跨模态对齐和零样本迁移能力较弱。为此，本文提出了PyraTok，一种语言对齐的金字塔分词器。它在预训练视频VAE基础上，通过新颖的语言对齐金字塔量化模块，在多个时空分辨率上利用共享大码本对特征进行离散化，生成紧凑且富有表现力的视频令牌序列。通过联合优化多尺度文本引导量化和令牌层次结构的全局自回归目标，PyraTok实现了视觉令牌与语言的紧密耦合。实验表明，PyraTok在视频重建、文本到视频生成以及视频理解等多项任务上均达到了最先进的性能。",
    "one_sentence": "本文提出了一种名为PyraTok的语言对齐金字塔视频分词器，通过多尺度共享大码本的量化方法，显著提升了视频表征与语言的对齐能力及下游任务性能。"
  }
]