[
  {
    "title": "RedSage: A Cybersecurity Generalist LLM",
    "url": "http://arxiv.org/abs/2601.22159v1",
    "abstract_en": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
    "date": "2026-01-29",
    "summary_cn": "为应对网络安全领域对兼具专业知识与数据隐私的LLM需求，本研究收集了11.8B网络安全语料进行领域预训练，并设计了模拟专家工作流的智能体增强管道生成了26.6万条监督微调数据。基于此训练的RedSage模型及配套评估基准RedSage-Bench在8B规模下，相较于基线模型在网络安全和通用评测中均有显著提升，证明了领域感知与智能体增强方法的有效性。所有模型、数据和代码均已开源。",
    "one_sentence": "本文提出了RedSage，一个通过大规模网络安全领域持续预训练与模拟专家工作流的智能体增强数据，在保证本地部署隐私性的同时，显著提升网络安全专业能力与通用推理性能的开源大语言模型。"
  },
  {
    "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
    "url": "http://arxiv.org/abs/2601.22158v1",
    "abstract_en": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
    "date": "2026-01-29",
    "summary_cn": "当前基于扩散或流的图像生成模型普遍依赖于多步采样和潜在空间操作。本研究提出像素均值流方法，其核心是将网络输出空间与损失空间分开设计：网络目标设定在预设的低维图像流形上，而损失则通过速度空间中的均值流定义。该方法在ImageNet 256x256和512x512分辨率上实现了高质量的一步式、无潜在空间生成，填补了该领域的关键空白，并推动了生成模型的边界。",
    "one_sentence": "本文提出了一种名为‘像素均值流’的方法，通过分离网络输出空间与损失空间的设计，实现了无需潜在空间的一步式扩散/流生成模型。"
  },
  {
    "title": "Discovering Hidden Gems in Model Repositories",
    "url": "http://arxiv.org/abs/2601.22157v1",
    "abstract_en": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.",
    "date": "2026-01-29",
    "summary_cn": "本研究指出，在公开模型库中，大量性能优越的微调模型因知名度低而被忽略，而社区热度并不能准确反映模型质量。通过对超过2000个模型的大规模评估，研究者发现许多“隐藏瑰宝”模型的性能远超热门模型，例如在Llama-3.1-8B家族中，一些冷门检查点能显著提升数学推理能力。为解决海量模型评估的计算难题，论文将模型发现问题建模为多臂老虎机问题，并提出了一种利用共享查询集和激进淘汰策略的加速搜索方法，可将发现效率提升超过50倍。",
    "one_sentence": "本文发现开源模型库中存在大量被忽视的高性能“隐藏瑰宝”模型，并针对此类模型的筛选问题，提出一种基于多臂老虎机框架和共享查询集的快速发现算法。"
  },
  {
    "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
    "url": "http://arxiv.org/abs/2601.22156v1",
    "abstract_en": "Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data",
    "date": "2026-01-29",
    "summary_cn": "针对现有方法将Transformer转换为混合注意力模型时所需数据量大且长上下文性能差的问题，本文提出了HALO蒸馏流程和新型混合架构HypeNet。HypeNet通过创新的位置编码（HyPE）和结构改进，实现了优异的长上下文泛化能力。实验将Qwen3系列模型转换为HypeNet，仅需极少数据（约23亿token），便取得了与原模型相当的性能，同时在长上下文场景下展现出更优的效率和效果。",
    "one_sentence": "本文提出了一种名为HALO的低成本转换方法，能将Transformer模型高效蒸馏为性能相当但长上下文推理更快的混合注意力模型HypeNet。"
  },
  {
    "title": "UEval: A Benchmark for Unified Multimodal Generation",
    "url": "http://arxiv.org/abs/2601.22155v1",
    "abstract_en": "We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.",
    "date": "2026-01-29",
    "summary_cn": "本文提出UEval基准，通过1000个专家设计的问题评估统一模型的多模态生成能力。为解决开放式评估难题，设计了基于规则的系统，利用多模态大模型生成并由专家验证评分细则，共包含一万余条标准。实验发现当前模型得分不高，推理模型表现更佳，且推理能力对于复杂任务至关重要。",
    "one_sentence": "本文提出了一个包含基于规则评分系统的UEval基准，用于全面评估图像和文本生成的统一模型。"
  }
]