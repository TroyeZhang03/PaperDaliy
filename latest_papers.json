[
  {
    "title": "Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models",
    "url": "http://arxiv.org/abs/2511.23478v1",
    "abstract_en": "Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.",
    "date": "2025-11-28",
    "summary_cn": "多模态大语言模型在处理动态视觉内容推理时，常出现逻辑不一致或依赖文本先验而非视觉证据的问题。为此，研究提出了两个诊断指标（TAC和VAS）来衡量推理质量，并设计了一种强化学习方法，通过时间感知监督微调和组相对策略优化来提升模型的时间对齐与因果连贯性。最终模型Video R2在多个基准测试中显著提升了准确性和可信度。",
    "one_sentence": "本文提出了一种结合时间感知监督微调和强化学习的后训练方法，通过提升时间对齐和推理一致性来改进视频理解模型。"
  },
  {
    "title": "Video-CoM: Interactive Video Reasoning via Chain of Manipulations",
    "url": "http://arxiv.org/abs/2511.23477v1",
    "abstract_en": "Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still \"think about videos\" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to \"think with videos\". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM",
    "date": "2025-11-28",
    "summary_cn": "传统多模态大语言模型处理视频时通常将视觉信息视为静态背景，导致细粒度时空推理能力不足。为此，本研究提出了Interactive Video Reasoning新范式，其核心模型Video CoM能够通过执行一系列视觉操作来主动分析和精炼视频证据。研究还构建了专用的指令调优数据集，并采用结合步骤级推理奖励的强化学习方法进行优化。实验表明，该模型在九个视频推理基准测试中表现优异，仅需少量数据即可显著超越现有先进模型，同时提高了推理的可解释性。",
    "one_sentence": "本文提出了一种名为Interactive Video Reasoning的新范式及Video CoM模型，通过让模型在视频上进行主动的视觉操作链式推理，突破了传统视频理解中的语义瓶颈。"
  },
  {
    "title": "Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction",
    "url": "http://arxiv.org/abs/2511.23476v1",
    "abstract_en": "Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.",
    "date": "2025-11-28",
    "summary_cn": "针对现有方法因僵化的推理过程而限制大语言模型主动学习的问题，本文探索了通过高效交互与主动推理实现世界模型内化的方法WMAct。该方法摒弃了结构化推理，通过奖励重标机制激励减少冗余行动，并采用交互频率退火策略迫使模型浓缩学习内容。实验表明，WMAct能在单次交互中解决以往需多次交互的任务，并在复杂环境中展现出强大的可迁移性，提升了推理性能。",
    "one_sentence": "本文提出了一种名为WMAct的方法，通过奖励重标和交互频率退火机制，使大语言模型在主动推理中内化世界模型，从而减少冗余交互并提升效率。"
  },
  {
    "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
    "url": "http://arxiv.org/abs/2511.23475v1",
    "abstract_en": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
    "date": "2025-11-28",
    "summary_cn": "多说话人视频生成面临数据采集成本高和多人交互协调难的问题。为此，本文提出AnyTalker框架，其核心创新在于扩展了Diffusion Transformer的注意力机制，引入身份感知模块以迭代处理身份-音频对，从而支持任意数量说话人的驱动。该框架的训练仅需单人视频即可学习说话模式，并仅用少量真实多人片段微调交互性。此外，作者还提出了专门的评估指标和数据集。实验表明，AnyTalker在唇音同步、视觉质量和交互自然度方面表现优异，在数据成本和身份扩展性之间取得了良好平衡。",
    "one_sentence": "本文提出了一种名为AnyTalker的可扩展多流处理框架，通过身份感知注意力机制和仅需单人视频的训练流程，实现了高质量、交互自然的多说话人视频生成。"
  },
  {
    "title": "ThetaEvolve: Test-time Learning on Open Problems",
    "url": "http://arxiv.org/abs/2511.23473v1",
    "abstract_en": "Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve",
    "date": "2025-11-28",
    "summary_cn": "针对现有闭源系统AlphaEvolve依赖前沿大模型且无法内部化进化策略的不足，本文提出了开源框架ThetaEvolve。它通过集成大规模程序数据库、批量采样、惰性惩罚等机制，支持在测试时进行高效的上下文学习和强化学习，使模型能从经验中持续学习。实验表明，该框架能让小型开源模型在多个开放任务上取得新的最佳边界，并且经过RL训练的模型展现出更强的进化能力和泛化性。",
    "one_sentence": "本文提出了一个开源的ThetaEvolve框架，使小型开源模型能通过测试时的强化学习持续进化，在开放优化问题上取得新的最佳边界。"
  }
]