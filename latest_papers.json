[
  {
    "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
    "url": "http://arxiv.org/abs/2511.21692v1",
    "abstract_en": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
    "date": "2025-11-26",
    "summary_cn": "本研究系统评估了大语言模型在不同难度任务上的泛化能力。与以往依赖人类判断不同，作者利用数千个LLMs的输出和项目反应理论，为六个数据集中的样本提供了客观、大规模的细粒度难度评级。研究发现，模型在难度跨度上的泛化能力有限，仅使用简单或困难数据进行训练无法在所有难度级别上取得一致的提升。这凸显了在模型训练和评估中使用涵盖多种难度数据的重要性。",
    "one_sentence": "本文创新地利用数千种大语言模型的输出和项目反应理论来客观评估任务难度，并系统分析了大模型在不同难度数据上的泛化能力。"
  },
  {
    "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls",
    "url": "http://arxiv.org/abs/2511.21691v1",
    "abstract_en": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.",
    "date": "2025-11-26",
    "summary_cn": "针对现有扩散模型在同时处理文本、参考主体、空间布局等多种控制信号时保真度不足的问题，本研究提出了Canvas-to-Image框架。其核心是将各类控制信息统一编码成一个画布图像，供模型进行综合的视觉-空间推理。通过构建多任务数据集和设计联合训练策略，模型学会了整合异构控制，而非依赖特定任务的启发式方法。实验表明，该方法在身份保持和控制遵循方面显著优于现有技术，尤其在多人合成、姿态控制等复杂场景下表现优异。",
    "one_sentence": "本文提出了一种名为Canvas-to-Image的统一框架，通过将多种异构控制信号编码到一张复合画布中，并采用多任务联合训练策略，使扩散模型能够综合理解和执行复杂的图像生成控制。"
  },
  {
    "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos",
    "url": "http://arxiv.org/abs/2511.21690v1",
    "abstract_en": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.",
    "date": "2025-11-26",
    "summary_cn": "针对机器人难以从少量演示中学习新任务的问题，本文引入了一种统一的3D轨迹空间符号表示法，以抽象外观差异并保留几何结构。基于此开发的TraceGen世界模型，通过TraceForge数据管道将异构视频转换为轨迹数据进行预训练，从而获得可迁移的运动先验知识。实验表明，仅需5个目标视频，该模型即可在多项任务中达到80%的成功率，并能快速适应来自不同形态（如手机拍摄的人类演示）的示教。",
    "one_sentence": "本文提出了一种名为TraceGen的世界模型，通过使用统一的3D轨迹空间符号表示，实现了利用少量跨平台、跨环境的演示视频高效学习机器人任务。"
  },
  {
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "url": "http://arxiv.org/abs/2511.21689v1",
    "abstract_en": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
    "date": "2025-11-26",
    "summary_cn": "该研究介绍了ToolOrchestra方法，通过训练小型编排模型来管理和协调多种智能工具。该方法采用结合结果、效率和用户偏好的强化学习奖励机制，最终得到一个80亿参数的Orchestrator模型。该模型在Humanity's Last Exam等复杂任务上，不仅以更低成本超越了GPT-5等大型模型的性能，还能根据用户偏好选择工具，实现了性能与成本的最佳平衡，为实用化的工具增强推理系统开辟了新途径。",
    "one_sentence": "本文提出了一种名为ToolOrchestra的方法，通过强化学习训练小型编排模型来协调多种智能工具，在解决复杂任务时实现了比大型语言模型更高的准确性和更低的成本。"
  },
  {
    "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
    "url": "http://arxiv.org/abs/2511.21688v1",
    "abstract_en": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
    "date": "2025-11-26",
    "summary_cn": "现有视觉语言模型在空间智能方面存在不足，主要原因是缺乏从2D图像重建3D空间的几何学习过程。为此，本文提出G²VLM模型，它将空间3D重建和空间理解这两个关键方面统一起来。该模型利用从多视角图像和视频数据中学到的3D视觉特征，不仅能直接预测3D属性，还能通过上下文学习和交错推理来提升空间推理任务的性能。实验表明，G²VLM在3D重建和空间理解任务上均取得了优异或具有竞争力的结果，为未来应用如3D场景编辑奠定了基础。",
    "one_sentence": "本文提出了G²VLM模型，通过将3D几何重建与空间理解相结合，利用学习到的3D视觉几何特征来增强视觉语言模型的空间推理能力。"
  }
]