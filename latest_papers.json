[
  {
    "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
    "url": "http://arxiv.org/abs/2512.25075v1",
    "abstract_en": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot",
    "date": "2025-12-31",
    "summary_cn": "SpaceTimePilot是一种视频扩散模型，能够独立改变相机视角和运动序列，实现场景在空间和时间上的连续探索。通过引入动画时间嵌入机制和时序扭曲训练方案，模型实现了时空解耦。此外，改进的相机调节机制和合成的CamxTime数据集进一步提升了控制精度。实验结果表明，该模型在真实和合成数据上均表现出色。",
    "one_sentence": "本文提出了SpaceTimePilot，一种通过解耦空间和时间实现可控生成渲染的视频扩散模型。"
  },
  {
    "title": "Randomization Times under Quantum Chaotic Hamiltonian Evolution",
    "url": "http://arxiv.org/abs/2512.25074v1",
    "abstract_en": "Randomness generation through quantum-chaotic evolution underpins foundational questions in statistical mechanics and applications across quantum information science, including benchmarking, tomography, metrology, and demonstrations of quantum computational advantage. While statistical mechanics successfully captures the temporal averages of local observables, understanding randomness at the level of higher statistical moments remains a daunting challenge, with analytic progress largely confined to random quantum circuit models or fine-tuned systems exhibiting space-time duality. Here we study how much randomness can be dynamically generated by generic quantum-chaotic evolution under physical, non-random Hamiltonians. Combining theoretical insights with numerical simulations, we show that for broad classes of initially unentangled states, the dynamics become effectively Haar-random well before the system can ergodically explore the physically accessible Hilbert space. Both local and highly nonlocal observables, including entanglement measures, equilibrate to their Haar expectation values and fluctuations on polynomial timescales with remarkably high numerical precision, and with the fastest randomization occurring in regions of parameter space previously identified as maximally chaotic. Interestingly, this effective randomization can occur on timescales linear in system size, suggesting that the sub-ballistic growth of Renyi entropies typically observed in systems with conservation laws can be bypassed in non-random Hamiltonians with an appropriate choice of initial conditions.",
    "date": "2025-12-31",
    "summary_cn": "本文研究了非随机哈密顿量下通用量子混沌演化产生的随机性。结合理论与数值模拟，发现对于广泛的初始无纠缠态，系统在远小于遍历希尔伯特空间的时间内即达到有效的哈尔随机性。局域和非局域可观测量均快速平衡，且随机化最快出现在最大混沌区域。值得注意的是，该过程可在系统规模的线性时间尺度上发生，表明通过适当选择初始条件可绕过守恒定律导致的熵次弹道增长限制。",
    "one_sentence": "本文揭示了在非随机哈密顿量驱动的通用量子混沌演化中，初始无纠缠态能在远小于遍历希尔伯特空间的时间内实现有效的哈尔随机性，且该过程可在系统规模的线性时间尺度上完成。"
  },
  {
    "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
    "url": "http://arxiv.org/abs/2512.25073v1",
    "abstract_en": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a $25\\times$ speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/",
    "date": "2025-12-31",
    "summary_cn": "针对稀疏视角3D重建的挑战，本文提出了GaMO框架，通过多视角外扩技术替代传统的新视角生成方法。该方法利用多视角条件和几何感知去噪策略，在无需训练的情况下实现了几何一致性和更广的场景覆盖。实验表明，GaMO在Replica和ScanNet++数据集上取得了先进的重建质量，并在速度上比现有扩散模型提升25倍，处理时间不到10分钟。",
    "one_sentence": "本文提出了GaMO框架，通过在现有相机位姿下扩展视场而非生成新视角，实现了零样本训练下的几何一致性保持和更广场景覆盖，显著提升了稀疏视角重建的质量与效率。"
  },
  {
    "title": "Edit3r: Instant 3D Scene Editing from Sparse Unposed Images",
    "url": "http://arxiv.org/abs/2512.25071v1",
    "abstract_en": "We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM2-based recoloring strategy that generates reliable, cross-view-consistent supervision, and (ii) an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views, encouraging the network to fuse and align disparate observations. At inference, our model effectively handles images edited by 2D methods such as InstructPix2Pix, despite not being exposed to such edits during training. For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total. Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.",
    "date": "2025-12-31",
    "summary_cn": "Edit3r是一种前馈框架，能够从无姿态、视角不一致的指令编辑图像中单次重建和编辑3D场景，无需逐场景优化或姿态估计。针对训练中缺乏多视角一致编辑图像的问题，提出了基于SAM2的重着色策略和非对称输入策略。实验表明，Edit3r在语义对齐和3D一致性方面优于基线方法，且推理速度显著提升，适用于实时3D编辑应用。",
    "one_sentence": "本文提出了Edit3r，一种无需逐场景优化即可从无姿态、视角不一致的指令编辑图像中直接重建和编辑3D场景的前馈框架。"
  },
  {
    "title": "Coordinated Humanoid Manipulation with Choice Policies",
    "url": "http://arxiv.org/abs/2512.25072v1",
    "abstract_en": "Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments.",
    "date": "2025-12-31",
    "summary_cn": "针对人形机器人全身协调控制的挑战，本文提出了一种结合模块化遥操作接口与可扩展学习框架的系统。通过分解控制模块高效收集数据，并引入Choice Policy生成多候选动作并评分，实现快速推理与多模态行为建模。实验表明，该方法在洗碗和擦白板等任务中显著优于现有方法，且手眼协调对长周期任务至关重要。",
    "one_sentence": "本文提出了一种结合模块化遥操作接口与可扩展学习框架的系统，通过Choice Policy生成多候选动作并评分，实现了人形机器人在复杂任务中的全身协调控制。"
  }
]