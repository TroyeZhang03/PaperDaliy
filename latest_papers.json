[
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "url": "http://arxiv.org/abs/2512.10959v1",
    "abstract_en": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
    "date": "2025-12-11",
    "summary_cn": "该论文介绍了一种名为StereoSpace的创新框架，它基于扩散模型实现从单目图像到立体图像的合成。其核心在于仅通过视角条件来隐式建模几何关系，无需依赖显式的深度估计或图像扭曲操作。该方法利用一个规范的矫正空间和条件引导，端到端地推断像素对应关系并填充因视角变化产生的遮挡区域。为确保评估的公平性，论文还提出了一套严格的端到端评测协议，并采用了侧重于感知舒适度和几何一致性的新指标。实验表明，StereoSpace在复杂场景下超越了多种现有方法，证明了视角条件扩散模型是一种可扩展且无需深度的立体生成解决方案。",
    "one_sentence": "本文提出了StereoSpace，一种仅通过视角条件建模几何、无需显式深度或形变的扩散模型框架，用于实现单目到立体的图像合成。"
  },
  {
    "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
    "url": "http://arxiv.org/abs/2512.10958v1",
    "abstract_en": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
    "date": "2025-12-11",
    "summary_cn": "针对生成式世界模型缺乏统一评估标准的问题，本文提出了WorldLens基准测试。该基准从生成、重建、行动跟随、下游任务和人类偏好五个维度，综合评价生成世界的视觉真实感、几何一致性、物理合理性和功能可靠性。为弥合客观指标与人类判断的差距，研究还构建了包含人工标注的WorldLens-26K数据集，并开发了可扩展、可解释的自动评估代理WorldLens-Agent，共同形成一个衡量世界保真度的统一生态系统。",
    "one_sentence": "本文提出了一个名为WorldLens的全方位基准测试，用于评估生成世界模型在构建、理解和行为交互方面的保真度，并配套发布了标注数据集和自动化评估代理以支持可扩展的评估。"
  },
  {
    "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
    "url": "http://arxiv.org/abs/2512.10957v1",
    "abstract_en": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
    "date": "2025-12-11",
    "summary_cn": "针对现有方法在严重遮挡和开放集环境下难以同时生成高质量几何与精确位姿的问题，本研究提出了SceneMaker解耦框架。该框架首先将去遮挡模型与3D物体生成解耦，并利用图像数据和收集的去遮挡数据集增强其对多样化遮挡模式的泛化能力。其次，提出一个统一位姿估计模型，整合了全局与局部注意力机制以提升精度。此外，还构建了开放集3D场景数据集以增强模型泛化性。大量实验证明，该框架在室内及开放集场景中均表现优越。",
    "one_sentence": "本文提出了一种名为SceneMaker的解耦式3D场景生成框架，通过分离去遮挡模型与3D物体生成，并引入结合全局与局部注意力的统一位姿估计模型，有效解决了严重遮挡和开放集场景下的几何质量与位姿精度问题。"
  },
  {
    "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
    "url": "http://arxiv.org/abs/2512.10956v1",
    "abstract_en": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
    "date": "2025-12-11",
    "summary_cn": "本文针对机器人导航基础模型（NFMs）在单目视觉下因深度尺度模糊和缺乏几何理解而效率低下的问题，提出了StereoWalker模型。该模型利用立体视觉输入解决深度模糊，并集成显式中层视觉先验（如深度估计与跟踪），以增强动态场景中的空间推理能力。同时，作者构建了一个大型立体导航数据集用于训练。实验表明，StereoWalker仅需1.5%的训练数据即可达到现有最佳性能，使用全数据时更优，且立体视觉显著优于单目输入。",
    "one_sentence": "本文提出了StereoWalker，通过引入立体视觉输入和显式中层视觉模块（如深度估计和稠密像素跟踪），显著提升了机器人导航基础模型的效率和性能。"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "url": "http://arxiv.org/abs/2512.10955v1",
    "abstract_en": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
    "date": "2025-12-11",
    "summary_cn": "视觉概念个性化旨在将特定图像属性（如身份、表情、光照和风格）转移到新场景中，但现有方法依赖通用图像编码器的整体嵌入，导致多个视觉因素纠缠，难以隔离单一属性，常引发信息泄漏和不连贯合成。为解决此问题，本文提出了Omni-Attribute，首个开放词汇图像属性编码器，通过策划带有正负属性标注的语义关联图像对，并采用平衡生成保真度与对比解缠的双重训练范式，学习高保真、属性特定的表示。实验表明，该编码器在开放词汇属性检索、个性化和组合生成中表现优异，在多个基准测试中达到最先进性能。",
    "one_sentence": "本文提出了首个开放词汇的图像属性编码器Omni-Attribute，通过联合设计数据和模型，利用语义关联的图像对与双重训练目标，实现了高保真、属性特定的表示学习。"
  }
]