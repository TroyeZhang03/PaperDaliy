[
  {
    "title": "RubricRL: Simple Generalizable Rewards for Text-to-Image Generation",
    "url": "http://arxiv.org/abs/2511.20651v1",
    "abstract_en": "Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.",
    "date": "2025-11-25",
    "summary_cn": "强化学习在文本到图像生成模型与人类偏好对齐方面前景广阔，但设计有效且可解释的奖励函数是一大挑战。现有方法多依赖于固定权重的复合指标或单一标量奖励，限制了其灵活性与可解释性。为此，本文提出RubricRL框架，它为每个输入提示动态生成一个结构化的评估标准清单，包含对象正确性、属性准确性等细粒度视觉标准。每个标准由多模态模型独立评估，并通过自适应权重机制突出关键维度。该方法不仅为策略优化提供了可解释的监督信号，还允许用户直接调整奖励侧重点。实验表明，RubricRL在提升生成图像的提示忠实度、视觉细节和泛化能力方面表现优异。",
    "one_sentence": "本文提出了RubricRL框架，通过为每个提示动态构建结构化的、可分解的细粒度视觉标准清单来设计奖励，从而提升强化学习对齐文本到图像模型的解释性和可控性。"
  },
  {
    "title": "MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities",
    "url": "http://arxiv.org/abs/2511.20650v1",
    "abstract_en": "Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.",
    "date": "2025-11-25",
    "summary_cn": "传统医学影像目标检测模型受限于封闭集范式，无法识别新类别目标。为解决此问题，本文首次提出医学影像实时开放词汇检测模型MedROV。研究构建了包含60万样本的大规模多模态数据集Omnis，并提出伪标签策略处理缺失标注。通过结合对比学习与大模型先验知识，MedROV显著提升了对已知及新类别结构的检测能力。实验表明，其性能远超现有医学检测基础模型和封闭集检测器，且检测速度达到70 FPS，为医学目标检测设立了新标杆。",
    "one_sentence": "本文提出了首个用于医学影像的实时开放词汇检测模型MedROV，通过构建大规模数据集和利用基础模型知识，有效检测已知和新出现的解剖结构。"
  },
  {
    "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
    "url": "http://arxiv.org/abs/2511.20649v1",
    "abstract_en": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
    "date": "2025-11-25",
    "summary_cn": "本文针对现有自回归视频扩散模型存在的时间跨度有限、长视频生成中提示响应慢、以及无法实现场景切换三大瓶颈，提出了统一的推理时解决方案∞-RoPE。该框架包含三个核心组件：块相对RoPE通过局部相对参考系实现超长视频生成；KV Flush通过精简KV缓存确保对提示的快速响应；RoPE Cut则能在单次生成中实现场景切换。实验表明，该方法在整体视频质量评估上优于现有模型。",
    "one_sentence": "本文提出了一个名为∞-RoPE的推理时框架，通过块相对RoPE、KV刷新和RoPE截断三项技术，解决了自回归视频扩散模型在生成长视频时面临的时间跨度、提示响应速度和场景切换能力三个核心瓶颈。"
  },
  {
    "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization",
    "url": "http://arxiv.org/abs/2511.20647v1",
    "abstract_en": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.",
    "date": "2025-11-25",
    "summary_cn": "针对现有文本生成视频模型在单一提示下输出多样性不足的问题，本文提出了DPP-GRPO框架。该框架创新性地融合了行列式点过程理论和群组相对策略优化，通过DPP对冗余样本施加递减回报、通过GRPO提供组级反馈，从而将多样性转化为显式优化信号。该方法无需改动模型结构即可有效提升视频在视觉外观、镜头运动和场景结构上的多样性，同时保持提示忠实度和生成质量，在多个基准测试中均取得了显著效果。",
    "one_sentence": "本文提出了一种结合行列式点过程和群组相对策略优化的新框架DPP-GRPO，将多样性作为显式优化目标，以解决文本生成视频任务中输出结果单一化的问题。"
  },
  {
    "title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight",
    "url": "http://arxiv.org/abs/2511.20648v1",
    "abstract_en": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.",
    "date": "2025-11-25",
    "summary_cn": "当前视觉语言模型擅长2D识别，但缺乏3D检测能力。本文提出的LocateAnything3D方法将3D检测视为下一个令牌预测任务，其核心是模仿人类推理过程的链式视觉序列：先在2D图像中定位物体，再依次推断距离、尺寸和姿态。该方法采用由易到难的训练策略，在Omni3D基准测试中取得了49.89 AP_3D的突破性成绩，比最佳基线提升15.51，并展现出优秀的零样本泛化能力。",
    "one_sentence": "本文提出了一种名为LocateAnything3D的新方法，将3D物体检测转化为视觉语言模型的下一个令牌预测问题，通过引入链式视觉推理序列实现了开放词汇和视觉提示下的最先进性能。"
  }
]