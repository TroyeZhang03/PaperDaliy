[
  {
    "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
    "url": "http://arxiv.org/abs/2602.20161v1",
    "abstract_en": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
    "date": "2026-02-23",
    "summary_cn": "本文提出了一种名为Mobile-O的紧凑型视觉-语言-扩散模型，旨在解决现有统一多模态模型计算量大、难以部署于移动设备的问题。其核心Mobile Conditioning Projector通过高效的深度可分离卷积和对齐设计，以极低计算成本实现了跨模态特征融合。模型仅需少量数据训练，并在创新的四元组数据格式下联合提升了视觉理解与生成能力。实验表明，Mobile-O在多项基准测试中性能优越，且在iPhone上能以约3秒生成一张512x512图像，首次为边缘设备提供了实用的实时多模态理解与生成框架。",
    "one_sentence": "本文提出了Mobile-O，一种通过高效的Mobile Conditioning Projector模块，首次在移动设备上实现实时、高效、端到端的多模态视觉理解与生成的紧凑型视觉-语言-扩散模型。"
  },
  {
    "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
    "url": "http://arxiv.org/abs/2602.20160v1",
    "abstract_en": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
    "date": "2026-02-23",
    "summary_cn": "本文提出tttLRM，一个创新的大型三维重建模型。它利用测试时训练层，以线性计算复杂度实现长上下文的自回归三维重建，并将多幅图像观测高效压缩至该层的快速权重中，形成一个可解码为多种显式格式（如高斯溅射）的隐式三维表示。该模型支持从流式观测中进行渐进式重建与优化。研究表明，模型在视图合成任务上的预训练能有效迁移至显式三维建模，提升重建质量并加速收敛。大量实验证明，该方法在物体和场景的前馈式三维高斯重建上优于现有先进技术。",
    "one_sentence": "本文提出了一种名为tttLRM的新型大尺度三维重建模型，它通过引入测试时训练层，实现了具有线性计算复杂度的长上下文自回归三维重建，并利用快速权重形成隐式表示以解码为多种显式格式。"
  },
  {
    "title": "A Very Big Video Reasoning Suite",
    "url": "http://arxiv.org/abs/2602.20159v1",
    "abstract_en": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
    "date": "2026-02-23",
    "summary_cn": "当前视频模型研究重画质轻推理。为解决大规模训练数据缺失的问题，本文提出了超大规模视频推理数据集VBVR，它覆盖200类任务、超百万视频片段。同时，作者构建了可验证的评测框架VBVR-Bench，采用基于规则、与人类对齐的评分器。利用此套件进行的规模化研究表明，模型对未见任务展现出了早期泛化迹象。VBVR为通用视频推理研究的下一阶段奠定了基础。",
    "one_sentence": "本文提出了一个前所未有的超大规模视频推理数据集VBVR及配套的可验证评测框架VBVR-Bench，首次系统研究了视频推理能力的规模化行为并观察到了泛化迹象。"
  },
  {
    "title": "Generalized $\\mathbb{Z}_p$ toric codes as qudit low-density parity-check codes",
    "url": "http://arxiv.org/abs/2602.20158v1",
    "abstract_en": "We study two-dimensional translation-invariant CSS stabilizer codes over prime-dimensional qudits on the square lattice under twisted boundary conditions, generalizing the Kitaev $\\mathbb{Z}_p$ toric code by augmenting each stabilizer with two additional qudits. Using the Laurent-polynomial formalism, we adapt the Gröbner basis to compute the logical dimension $k$ efficiently, without explicitly constructing large parity-check matrices. We then perform a systematic search over various stabilizer realizations and lattice geometries for $p\\in\\{3,5,7,11\\}$, identifying qudit low-density parity-check codes with the optimal finite-size performance. Representative examples include $[[242,10,22]]_3$ and $[[120,6,20]]_{11}$, both achieving $k d^{2}/n=20$. Across the searched regime, the best observed $k d^{2}$ at fixed $n$ increases with $p$, with an empirical relation $k d^{2} = 0.0541 \\, n^{2}\\ln p + 3.84 \\, n$, compatible with a Bravyi--Poulin--Terhal-type tradeoff when the interaction range grows with system size.",
    "date": "2026-02-23",
    "summary_cn": "本研究在二维周期性边界条件下，推广了Kitaev的Z_p环面码，为每个稳定子增加了两个额外的量子比特。利用洛朗多项式形式和格罗布纳基方法，高效计算了逻辑维度k，避免了大型奇偶校验矩阵的显式构造。通过对不同稳定子实现和晶格几何进行系统搜索（针对p=3,5,7,11），识别出具有最优有限尺寸性能的量子低密度奇偶校验码，例如[[242,10,22]]_3和[[120,6,20]]_11。结果表明，最佳性能参数k*d^2随p增大而提升，且与系统尺寸相关的相互作用范围增长时，符合Bravyi–Poulin–Terhal型权衡关系。",
    "one_sentence": "本文通过推广Kitaev环面码并引入格罗布纳基方法，系统搜索并识别出具有最优有限尺寸性能的高效量子纠错码。"
  },
  {
    "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
    "url": "http://arxiv.org/abs/2602.20157v1",
    "abstract_en": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.",
    "date": "2026-02-23",
    "summary_cn": "现有三维/四维重建系统依赖难以大规模获取的稠密几何与姿态标注。本文提出Flow3r框架，利用无标签单目视频中预测的稠密二维对应关系（光流）进行监督。其核心创新在于将光流预测模块因式分解：使用一个图像的几何隐变量和另一个图像的姿态隐变量来预测两者间的光流。这一分解能直接引导场景几何和相机运动的学习，并自然扩展到动态场景。实验表明，该方法在包含静态和动态场景的八个基准测试中取得了最先进的结果，尤其在标注数据稀缺的野外动态视频上提升显著。",
    "one_sentence": "本文提出Flow3r框架，通过引入因式分解的稠密光流预测作为监督信号，能够在无需几何与姿态标签的单目视频上可扩展地学习静态和动态场景的三维几何与相机运动。"
  }
]