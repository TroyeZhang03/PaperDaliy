[
  {
    "title": "Dataset Distillation for Pre-Trained Self-Supervised Vision Models",
    "url": "http://arxiv.org/abs/2511.16674v1",
    "abstract_en": "The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.",
    "date": "2025-11-20",
    "summary_cn": "本文研究针对大型预训练视觉模型的数据集蒸馏问题，旨在合成少量图像以优化线性分类器的训练。提出的线性梯度匹配方法，通过使合成数据在预训练特征提取器中产生的线性分类器梯度与真实数据相似，实现了优于真实图像基线的性能。该方法具有跨模型通用性，在细粒度分类和模型可解释性方面表现出色。",
    "one_sentence": "本文提出了一种名为线性梯度匹配的数据集蒸馏方法，针对预训练模型优化合成图像，使得线性分类器在其上的梯度与真实数据一致。"
  },
  {
    "title": "EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards",
    "url": "http://arxiv.org/abs/2511.16672v1",
    "abstract_en": "Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\\sim$3\\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.",
    "date": "2025-11-20",
    "summary_cn": "针对现有大型多模态模型依赖人工标注数据的问题，本文提出了EvoLMM自进化框架。该框架从单一骨干模型衍生出提议和求解两个协作智能体：提议者生成多样化的图像相关问题，求解者通过内部一致性进行解答，并通过自我奖励机制持续学习。这种纯无监督方法在多个数学推理基准测试上取得了显著提升，为自改进多模态模型研究提供了新思路。",
    "one_sentence": "本文提出了一种名为EvoLMM的自进化框架，通过在同一基础模型中实例化提议者和求解者两个协作智能体，实现了无需人工标注或奖励模型的纯无监督多模态大模型推理能力提升。"
  },
  {
    "title": "NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses",
    "url": "http://arxiv.org/abs/2511.16673v1",
    "abstract_en": "We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate \"ground-truth\" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).",
    "date": "2025-11-20",
    "summary_cn": "现有方法在从单张或稀疏图像重建可动画3D人体化身时，通常依赖精确的相机和人体姿态作为输入。本文指出，若姿态估计存在噪声，重建效果会大幅下降。为此，作者提出了NoPo-Avatar方法，它仅需图像即可完成重建，无需任何姿态输入。该方法消除了测试时对人体姿态的依赖，使其不受噪声影响，应用更广。在多个数据集上的实验表明，在无真实姿态的实际场景中，NoPo-Avatar优于现有基线方法；在有真实姿态的实验室场景中，也能取得相当的结果。",
    "one_sentence": "本文提出了一种名为NoPo-Avatar的新方法，无需输入人体姿态信息即可从单张或稀疏图像重建可动画的3D人体化身，有效克服了传统方法对精确姿态估计的依赖及其噪声干扰。"
  },
  {
    "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
    "url": "http://arxiv.org/abs/2511.16671v1",
    "abstract_en": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
    "date": "2025-11-20",
    "summary_cn": "当前视觉生成模型通常在生成前或生成后引入文本推理，而缺乏生成过程中的实时交互。本研究提出了一种名为TwiG的创新框架，首次实现了视觉生成与文本推理的交织进行。该框架让文本推理在图像逐步生成的同时介入，既能指导后续局部内容的生成，也能反思已生成的部分，从而产生更具上下文意识和语义丰富的视觉输出。为探索该框架潜力，研究还评估了零样本提示、有监督微调和强化学习三种策略，旨在推动交织式推理在视觉生成领域的进一步研究。",
    "one_sentence": "本文提出了首个在视觉生成过程中交织进行文本推理的框架TwiG，实现了推理与生成的同时动态演化。"
  },
  {
    "title": "Learning to Think Fast and Slow for Visual Language Models",
    "url": "http://arxiv.org/abs/2511.16670v1",
    "abstract_en": "When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.",
    "date": "2025-11-20",
    "summary_cn": "针对现有视觉语言模型在处理不同复杂度任务时均采用冗长推理链导致计算成本高的问题，本文提出DualMindVLM模型。该方法受人类双系统思维启发，通过一个两阶段训练过程：首先根据模型输出长度标注快慢思考模式，然后利用GRPO强化学习训练模型自主切换模式。实验表明，该模型在性能媲美先进方法的同时，实现了极高的计算效率。",
    "one_sentence": "本文提出了一种简单的强化学习方法，使视觉语言模型能根据任务难度自动切换快思考与慢思考模式，在保持高性能的同时显著提升计算效率。"
  }
]