[
  {
    "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
    "url": "http://arxiv.org/abs/2602.12281v1",
    "abstract_en": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
    "date": "2026-02-12",
    "summary_cn": "本文提出了一种名为CoVer的对比验证框架，用于缩小视觉-语言-行动（VLA）模型中的“意图-行动差距”。通过联合缩放指令重述和行动生成，CoVer在测试时显著提升了行动样本的多样性。实验表明，该方法在SIMPLER和PolaRiS基准上分别实现了22%和14%的性能提升，并在真实场景中进一步提高了45%的效果。",
    "one_sentence": "本文提出了CoVer框架，通过测试时验证和指令-行动联合缩放策略，显著缩小了视觉-语言-行动模型中的“意图-行动差距”。"
  },
  {
    "title": "Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching",
    "url": "http://arxiv.org/abs/2602.12280v1",
    "abstract_en": "Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the \"dual-constraint\": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a \"common structural subspace\" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/",
    "date": "2026-02-12",
    "summary_cn": "本文介绍了“渐进式语义幻觉”，一种通过逐笔添加笔画实现单一素描语义剧变的矢量绘图新任务。为此，作者提出了“Stroke of Surprise”生成框架，利用双分支分数蒸馏采样机制和新型叠加损失，动态优化笔画以解决双约束难题，使其在绘制初始和完成阶段分别呈现不同且连贯的物体，成功将视觉变位图扩展至时间维度。",
    "one_sentence": "本文提出了一种名为‘渐进式语义幻觉’的矢量素描新任务及生成框架，通过动态优化笔画序列，使单一素描在绘制过程的不同阶段呈现截然不同的语义内容。"
  },
  {
    "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
    "url": "http://arxiv.org/abs/2602.12279v1",
    "abstract_en": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
    "date": "2026-02-12",
    "summary_cn": "本文提出了UniT框架，旨在解决统一多模态模型无法进行迭代推理的局限。该方法结合智能体数据合成与统一训练，赋予模型验证、分解子目标和记忆的能力。研究表明，基于短推理链训练的模型能有效扩展至长推理，且顺序思维链比并行采样更具计算效率，显著提升了模型的生成与理解性能。",
    "one_sentence": "本文提出了UniT框架，首个实现统一多模态模型思维链测试时扩展的方法，通过迭代推理提升复杂任务的生成与理解能力。"
  },
  {
    "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
    "url": "http://arxiv.org/abs/2602.12278v1",
    "abstract_en": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.",
    "date": "2026-02-12",
    "summary_cn": "本文提出了一种名为AttentionRetriever的新型长文档检索模型，通过注意力机制和基于实体的检索方法，解决了长文档检索中的上下文感知、因果依赖和检索范围等关键问题。实验表明，该模型在长文档检索数据集上显著优于现有模型，同时保持了与稠密检索模型相当的效率。",
    "one_sentence": "本文提出了一种名为AttentionRetriever的新型长文档检索模型，通过注意力机制和基于实体的检索构建上下文感知嵌入，显著提升了长文档检索的性能。"
  },
  {
    "title": "Reionization Bubbles from Real-Space Cross Correlations of Line Intensity Maps",
    "url": "http://arxiv.org/abs/2602.12277v1",
    "abstract_en": "We propose a new way to reconstruct the ionized-bubble size distribution during the Epoch of Reionization (EoR) through the real-space cross-correlation of 21-cm and star-forming line-intensity maps. Understanding the evolution and timing of the EoR is crucial for both astrophysics and cosmology, and a wealth of information on the first sources can be extracted from the study of ionized bubbles. Nevertheless, directly mapping bubbles is challenging due to the high redshifts involved, possible selection biases, and foregrounds in 21-cm maps. Here, we exploit the real-space cross-correlation $ξ_{21,ν}$ between 21-cm and line-intensity mapping (LIM) signals to reconstruct the evolution of bubble sizes during reionization. For the first time, we show that $ξ_{21,ν}(r)$ departs from a saturation level for each separation $r$ when bubbles of size $r$ begin to form, providing a handle for the onset of bubbles of each radius. Moreover, we demonstrate that $ξ_{21,ν}$ evolves from positive to negative as the EoR progresses, reaching a minimum (i.e. maximum anti-correlation) when bubbles of radius $r$ reach peak abundance. We show that these results are robust to changes in the astrophysical model as well as the timing/topology of reionization. This real-space observable complements usual Fourier-space estimators by capturing the localized nature of bubbles, offering new insights into the sources driving cosmic reionization.",
    "date": "2026-02-12",
    "summary_cn": "本文提出了一种新方法，通过21厘米信号与恒星形成谱线强度图的实空间互相关，重构再电离时期的电离气泡大小分布。研究发现，互相关函数随气泡形成和演化呈现特征变化，能捕捉气泡的局域性质，为研究再电离驱动源提供了新视角。该方法对天体物理模型和再电离时序具有鲁棒性，补充了传统傅里叶空间估计器的不足。",
    "one_sentence": "本文提出了一种利用21厘米和恒星形成谱线强度图的实空间互相关重构再电离时期电离气泡大小分布的新方法。"
  }
]