[
  {
    "title": "TESS Planet Occurrence Rates Reveal the Disappearance of the Radius Valley Around Mid-to-Late M Dwarfs",
    "url": "http://arxiv.org/abs/2602.23364v1",
    "abstract_en": "We present the deepest systematic search for planets around mid-to-late M dwarfs to date. We have surveyed 8134 mid-to-late M dwarfs observed by TESS with a custom built pipeline and recover 77 vetted transiting planet candidates. We characterize the sensitivity of our survey via injection-recovery and measure the occurrence rate of planets as a function of orbital period, instellation, and planet radius. We measure a cumulative occurrence rate of $1.10\\pm0.16$ planets per star with radii $>1\\, R_\\oplus$ orbiting within 30 days. This value is consistent with the cumulative occurrence rate around early M dwarfs, making M dwarfs collectively the most prolific hosts of small close-in planets. Unlike the bimodal Radius Valley exhibited by close-in planet population around FGK and early M dwarfs, we recover a unimodal planet radius distribution peaking at $1.25\\pm0.05 \\, R_\\oplus$. We additionally find $0.954\\pm0.147$ super-Earths and $0.148\\pm0.045$ sub-Neptunes per star, with super-Earths outnumbering sub-Neptunes 5.5:1, firmly demonstrating that the Radius Valley disappears around the lowest mass stars. The dearth of sub-Neptunes around mid-to-late M dwarfs is consistent with predictions from water-rich pebble accretion models that predict a fading Radius Valley with decreasing stellar mass. Our results support the emerging idea that the sub-Neptune population around M dwarfs is composed of water-rich worlds. We find no hot Jupiters in our survey and set an upper limit of 0.012 hot Jupiters per mid-to-late M dwarf within 10 days.",
    "date": "2026-02-26",
    "summary_cn": "本研究利用 TESS 数据对 8134 颗中晚型 M 矮星进行迄今最深的行星搜索，发现 77 颗候选体。结果显示该类恒星是小行星的高效宿主，其行星半径分布呈单峰且“半径谷”消失，支持亚海王星为富水世界的理论，并设定了热木星的上限。",
    "one_sentence": "本文通过迄今对中晚型 M 矮星最深的系统性搜索，首次确证了该类型恒星周围行星半径分布呈单峰特征且“半径谷”消失，表明其亚海王星主要由富水世界组成。"
  },
  {
    "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
    "url": "http://arxiv.org/abs/2602.23363v1",
    "abstract_en": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com",
    "date": "2026-02-26",
    "summary_cn": "本文提出 MediX-R1，一种面向医疗多模态大模型的开放式强化学习框架。该方法利用基于组的 RL 和包含准确性、语义及格式的多重奖励信号，解决了传统方法在自由形式回答中的局限。配合基于 LLM 的统一评估体系，模型在少量数据下显著提升了开放临床任务的推理能力。",
    "one_sentence": "本文提出了 MediX-R1，一种专为医疗多模态大模型设计的开放式强化学习框架，通过组合式奖励机制和基于 LLM 的评估体系，实现了超越选择题格式的、具有临床依据的自由形式推理。"
  },
  {
    "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale",
    "url": "http://arxiv.org/abs/2602.23361v1",
    "abstract_en": "We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.",
    "date": "2026-02-26",
    "summary_cn": "本文提出 VGG-T³模型，通过测试时训练将场景几何的可变长度键值表示蒸馏为固定大小 MLP，使计算复杂度从二次降为线性。该方法在保持全局聚合能力的同时，大幅提升了重建速度与精度，并支持基于未见图像的视觉定位。",
    "one_sentence": "本文提出了一种基于测试时训练将可变长度键值空间蒸馏为固定大小 MLP 的方法，解决了离线前馈 3D 重建模型计算和内存需求随输入图像数量二次增长的问题。"
  },
  {
    "title": "Model Agreement via Anchoring",
    "url": "http://arxiv.org/abs/2602.23360v1",
    "abstract_en": "Numerous lines of aim to control $\\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.   We develop a simple general technique for proving bounds on independent model disagreement based on $\\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.",
    "date": "2026-02-26",
    "summary_cn": "本文针对机器学习中的模型分歧问题，提出了一种基于锚定两模型平均值的通用分析技术。该方法无需协调训练过程，即可证明随着堆叠数量、迭代次数或架构规模增加，四种主流算法的模型分歧趋于零，结果适用于任意强凸损失函数。",
    "one_sentence": "本文提出了一种基于模型平均锚定的通用技术，用于证明独立训练模型间的分歧界限，并成功将其应用于四种主流机器学习算法。"
  },
  {
    "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation",
    "url": "http://arxiv.org/abs/2602.23359v1",
    "abstract_en": "We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.",
    "date": "2026-02-26",
    "summary_cn": "针对现有方法在 3D 布局生成中忽视遮挡推理的问题，本文提出 SeeThrough3D 模型。该方法利用半透明 3D 框表示场景以编码遮挡信息，并结合掩码自注意力机制绑定对象与文本，有效生成了具有精确遮挡、深度一致且可控制视角的多对象场景。",
    "one_sentence": "本文提出了 SeeThrough3D 模型，通过引入编码遮挡信息的半透明 3D 场景表示和掩码自注意力机制，实现了具有精确遮挡关系和深度一致性的 3D 布局条件生成。"
  }
]