[
  {
    "title": "Unveiling the 3D structure of the central molecular zone from stellar kinematics and photometry: The 50 and 20 km/s clouds",
    "url": "http://arxiv.org/abs/2601.05252v1",
    "abstract_en": "The central molecular zone (CMZ), surrounding the Galactic centre, is the largest reservoir of dense molecular gas in the Galaxy. Despite its relative proximity, the 3D structure of the CMZ remains poorly constrained, primarily due to projection effects. We aim to constrain the line-of-sight location of two molecular clouds in the CMZ -- the 50 and 20 km/s clouds -- and to investigate their possible physical connection using stellar kinematics and photometry. This study serves as a pilot for future applications across the full CMZ. We estimated the line-of-sight position of the clouds by analysing stellar kinematics, stellar densities, and stellar populations towards the cloud regions and a control field. We find an absence of westward moving stars in the cloud regions, which indicates that they lie on the near side of the CMZ. This interpretation is supported by the stellar density distributions. The similar behaviour observed in the two clouds, as well as in the region between them (the ridge), suggests that they are located at comparable distances and are physically linked. We also identified an intermediate-age stellar population (2-7 Gyr) in both regions, consistent with that observed on the near side of the CMZ. We estimated the line-of-sight distances at which the clouds and the ridge become kinematically detectable (i.e. where the proper motion component parallel to the Galactic plane differs from that of the control field at the 3 sigma level) by converting their measured proper motions parallel to the Galactic plane using a theoretical model of the stellar distribution. We find that the 50 and 20 km/s clouds are located at $43\\pm8$ pc and $56\\pm11$ pc from Sgr A*, respectively, and that the ridge lies at $56\\pm11$ pc; this supports the idea that the clouds are physically connected through the ridge.",
    "date": "2026-01-08",
    "summary_cn": "本研究旨在利用恒星运动学与测光数据，约束银河系中心分子带内两个分子云（50和20 km/s云）的视线方向位置，并探究其物理关联。分析发现，这两个云及连接它们的脊均位于CMZ的近侧，距离银心Sgr A*约43至56秒差距，且拥有相似的中老年恒星族（2-7 Gyr）。结果表明它们位于可比距离并通过脊物理相连，为后续全面研究CMZ三维结构提供了方法验证。",
    "one_sentence": "本文通过分析恒星运动学与星族，首次将银河系中心分子带的两个分子云（50和20 km/s云）及其连接区域（脊）定位在银心附近，并证实了它们之间的物理联系。"
  },
  {
    "title": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video",
    "url": "http://arxiv.org/abs/2601.05251v1",
    "abstract_en": "We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.",
    "date": "2026-01-08",
    "summary_cn": "本文提出了Mesh4D模型，用于从单目视频中重建动态物体的完整三维形状与运动。其核心是学习一个紧凑的隐空间，该空间通过自编码器在训练时借助骨骼结构先验来编码整个动画序列，推理时则无需骨骼信息。模型利用时空注意力机制增强变形表示的稳定性，并基于此训练一个隐扩散模型，能够根据输入视频和首帧重建的网格，一次性预测出全部动画。实验表明，Mesh4D在三维形状和变形重建任务上优于现有方法。",
    "one_sentence": "本文提出Mesh4D模型，通过结合骨骼结构先验的紧凑隐空间编码和隐扩散模型，实现了从单目视频一次性预测完整动态三维网格序列。"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "url": "http://arxiv.org/abs/2601.05249v1",
    "abstract_en": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
    "date": "2026-01-08",
    "summary_cn": "夜间色彩恒常性因低光噪声和复杂光照而极具挑战。本文提出RL-AWB框架，首先设计了一种针对夜间场景的统计方法进行光照估计，并在此基础上首次引入深度强化学习，以该统计方法为核心，动态优化参数以模拟专业调校。此外，研究构建了首个多传感器夜间数据集以支持跨设备评估。实验表明，该方法在低光和正常光照图像上均表现出优异的泛化能力。",
    "one_sentence": "本文提出了一种结合统计方法与深度强化学习的夜间白平衡新框架RL-AWB，并构建了首个多传感器夜间数据集。"
  },
  {
    "title": "QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer",
    "url": "http://arxiv.org/abs/2601.05250v1",
    "abstract_en": "Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.",
    "date": "2026-01-08",
    "summary_cn": "本研究将量子视觉场（QVFs）的方法扩展到新视角合成任务，提出了首个混合量子-经典模型QNeRF。该模型利用参数化量子电路，通过量子叠加和纠缠来编码空间和视角信息，从而构建出比经典神经辐射场（NeRF）更紧凑的模型。论文提出了两种架构变体：充分利用量子振幅的全QNeRF，以及通过分支结构降低复杂度的双分支QNeRF。实验表明，在中等分辨率图像上训练时，QNeRF使用不到一半的参数即可达到或超越经典NeRF基线的性能，展示了量子机器学习在计算机视觉中等任务中的潜力。",
    "one_sentence": "本文首次提出了QNeRF，一种用于从2D图像进行新视角合成的混合量子-经典模型，通过参数化量子电路编码信息，在参数更少的情况下达到或超越经典方法性能。"
  },
  {
    "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model",
    "url": "http://arxiv.org/abs/2601.05248v1",
    "abstract_en": "Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0",
    "date": "2026-01-08",
    "summary_cn": "现有的视觉-语言-动作模型通常依赖显式的语言推理，这会导致延迟并难以捕捉物理动态。为解决此问题，本文提出了LaST₀框架。该框架在潜在空间中构建时空思维链，以隐式建模未来的视觉、3D结构和本体感知状态。同时，其采用混合Transformer的双系统架构，让推理专家和动作专家以不同频率协作，从而在保证高成功率的同时实现快速推理。实验表明，该方法在模拟和真实任务中均显著优于现有方法。",
    "one_sentence": "本文提出了一种名为LaST₀的框架，通过潜在时空思维链实现高效推理，并采用混合专家架构来平衡推理精度与动作频率。"
  }
]