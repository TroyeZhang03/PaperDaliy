[
  {
    "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
    "url": "http://arxiv.org/abs/2511.21692v1",
    "abstract_en": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
    "date": "2025-11-26",
    "summary_cn": "本研究针对大语言模型在不同难度任务上的泛化能力进行了系统性评估。与以往依赖人类判断不同，作者利用数千个LLM的输出和项目反应理论，对六个数据集中的样本进行了客观、细粒度的难度评级。研究发现，模型的跨难度泛化能力普遍有限，仅使用简单或困难数据训练无法在所有难度级别上取得一致提升。这强调了在模型训练和评估中使用涵盖全难度范围数据的重要性。",
    "one_sentence": "本文创新地利用数千种大语言模型和项目反应理论对任务难度进行大规模、细粒度的客观评估，揭示了模型跨难度泛化能力有限的关键发现。"
  },
  {
    "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls",
    "url": "http://arxiv.org/abs/2511.21691v1",
    "abstract_en": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.",
    "date": "2025-11-26",
    "summary_cn": "针对现有扩散模型在同时处理文本、参考主体、空间布局等多种控制信号时的高保真度难题，本文提出了Canvas-to-Image框架。其核心是将不同控制信息统一编码到一个复合画布上，并设计了多任务数据集和联合训练策略，使模型能够直接对画布进行视觉-空间推理。实验表明，该方法在身份保持和控制遵循方面显著优于现有技术，尤其在多人组合、姿态控制等复杂场景下表现出优异的泛化能力。",
    "one_sentence": "本文提出了一种名为Canvas-to-Image的统一框架，通过将多种异构控制信号编码到单一画布图像中，并采用多任务画布训练策略，使扩散模型能够联合理解并忠实生成符合复杂意图的图像。"
  },
  {
    "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos",
    "url": "http://arxiv.org/abs/2511.21690v1",
    "abstract_en": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.",
    "date": "2025-11-26",
    "summary_cn": "针对机器人从少量演示中学习新任务的挑战，本文引入了TraceGen模型。该模型的核心创新在于使用一种名为“trace-space”的3D符号化轨迹表示，它能从人类或不同机器人的异构视频中抽象出几何运动信息。通过大规模数据管道TraceForge构建的训练集进行预训练后，TraceGen获得了可迁移的3D运动先验知识。实验表明，仅需5个目标机器人演示视频，该模型就能在多项任务上达到80%的成功率，推理速度比现有视频模型快50-600倍；即使仅用手机拍摄的5段人类演示视频，其在真实机器人上的任务成功率也达到67.5%，展现了强大的跨载体适应能力。",
    "one_sentence": "本文提出了一种名为TraceGen的世界模型，它通过一种统一的3D轨迹空间符号表示法，实现了利用少量跨载体、跨环境的演示视频来高效学习机器人新任务。"
  },
  {
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "url": "http://arxiv.org/abs/2511.21689v1",
    "abstract_en": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
    "date": "2025-11-26",
    "summary_cn": "该论文介绍了ToolOrchestra方法，它通过强化学习训练小型编排模型来管理和协调多种智能工具。由此产生的Orchestrator模型（80亿参数）在Humanity's Last Exam等复杂任务上，以更低的成本超越了GPT-5等大型模型的性能，并在工具选择上更好地符合用户偏好。研究表明，这种轻量级编排模型与多样化工具组合的方法，在性能与成本之间实现了最佳平衡，为实用且可扩展的工具增强推理系统开辟了道路。",
    "one_sentence": "本文提出了一种名为ToolOrchestra的新方法，通过强化学习训练小型编排模型来协调多种智能工具，在降低计算成本的同时提升了解决复杂任务的能力。"
  },
  {
    "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
    "url": "http://arxiv.org/abs/2511.21688v1",
    "abstract_en": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
    "date": "2025-11-26",
    "summary_cn": "现有视觉语言模型在空间理解和推理任务上表现不佳，主要原因是缺乏从2D图像重建3D空间的几何学习能力。为此，本文提出了G²VLM模型，它统一了3D重建和空间理解这两个空间智能的核心方面。该模型能够利用学习到的3D几何特征直接预测3D属性，并通过上下文学习与交错推理提升空间推理能力。实验表明，G²VLM在3D重建任务上达到先进水平，在空间理解任务上也取得了优异或具有竞争力的结果，为未来应用（如3D场景编辑）提供了强大基础。",
    "one_sentence": "本文提出了一种名为G²VLM的几何基础视觉语言模型，通过将3D视觉几何学习与语义理解相结合来增强空间智能。"
  }
]