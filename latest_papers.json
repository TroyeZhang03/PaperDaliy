[
  {
    "title": "RedSage: A Cybersecurity Generalist LLM",
    "url": "http://arxiv.org/abs/2601.22159v1",
    "abstract_en": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
    "date": "2026-01-29",
    "summary_cn": "为构建私密、领域专业的网络安全大语言模型助手，本研究收集并精炼了海量网络安全领域数据用于持续预训练，并通过模拟专家工作流程生成高质量指令微调数据。基于此训练出的开源模型RedSage在多个网络安全及通用评测基准上显著超越了基线模型，证明了该方法能有效提升领域专长与通用能力。所有模型、数据与代码均已开源。",
    "one_sentence": "本文通过领域感知的持续预训练与基于工作流模拟的智能体增强，训练出高性能、可本地部署的开源网络安全助手RedSage。"
  },
  {
    "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
    "url": "http://arxiv.org/abs/2601.22158v1",
    "abstract_en": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
    "date": "2026-01-29",
    "summary_cn": "现代基于扩散/流的图像生成模型通常依赖多步采样和在潜在空间中操作。本文提出的“像素均值流”方法，通过将网络输出（定义为x-预测）置于低维图像流形，同时在速度空间通过均值流定义损失，并引入两者间的简单变换，实现了无需潜在空间的单步高效生成。实验表明，该方法在ImageNet 256x256和512x512分辨率上取得了优异的单步生成效果，填补了该领域的一个关键空白。",
    "one_sentence": "本文提出了一种名为“像素均值流”的方法，通过分离网络输出空间与损失空间设计，实现了在无潜在空间操作下的高效单步图像生成。"
  },
  {
    "title": "Discovering Hidden Gems in Model Repositories",
    "url": "http://arxiv.org/abs/2601.22157v1",
    "abstract_en": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.",
    "date": "2026-01-29",
    "summary_cn": "本研究指出，在公开模型仓库中，模型的使用高度集中在少数基础模型上，而大量优质的微调模型被忽视。通过对超过2000个模型进行广泛评估，作者发现了性能显著优于热门模型的“隐藏宝石”。针对模型搜索的计算成本问题，研究将模型发现形式化为多臂老虎机问题，并通过改进序列减半算法（如共享查询集和激进的淘汰机制）加速了搜索过程，实现了超过50倍的加速比。",
    "one_sentence": "本文发现大模型社区中存在被忽略的优质微调模型，并通过将其搜索过程建模为多臂老虎机问题，提出了一种高效的模型发现方法。"
  },
  {
    "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
    "url": "http://arxiv.org/abs/2601.22156v1",
    "abstract_en": "Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data",
    "date": "2026-01-29",
    "summary_cn": "现有将Transformer模型转换为RNN-注意力混合模型的方法存在训练数据需求大（>10B tokens）且长上下文性能不佳的问题。本文提出了HALO训练管道和包含新型位置编码HyPE的HypeNet混合架构，能以极少的训练数据（仅2.3B tokens）成功转换Qwen3系列模型，在保持原模型性能的同时，显著提升了长上下文建模的效率和效果。",
    "one_sentence": "本文提出了一种名为HALO的高效训练管道和名为HypeNet的混合模型架构，能以极少量数据（2.3B tokens）将预训练Transformer模型高效转换为兼具高性能和长上下文推理速度的RNN-注意力混合模型。"
  },
  {
    "title": "UEval: A Benchmark for Unified Multimodal Generation",
    "url": "http://arxiv.org/abs/2601.22155v1",
    "abstract_en": "We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.",
    "date": "2026-01-29",
    "summary_cn": "本文提出了UEval基准，用于评估能同时生成图像和文本的统一模型。该基准包含从8个真实任务中收集的1000个需图文结合回答的问题，覆盖了多种推理类型。为解决开放式多模态生成评估的难题，研究团队设计了基于量规的评分系统，即先由多模态大模型为每个问题生成初始评分标准，再经专家细化和验证，最终得到超过一万条标准。实验表明，当前领先模型在UEval上得分仍不理想，而推理能力对任务表现至关重要，将推理过程迁移至非推理模型可显著提升其性能。",
    "one_sentence": "本文提出了一个包含千余专家标注问题的统一模型评估基准UEval，并设计了基于详细量规的大语言模型自动评分系统。"
  }
]