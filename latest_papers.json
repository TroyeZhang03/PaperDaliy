[
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "url": "http://arxiv.org/abs/2512.10959v1",
    "abstract_en": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
    "date": "2025-12-11",
    "summary_cn": "本文介绍StereoSpace，这是一个基于扩散的单目到立体合成框架。它仅依赖视点条件来建模几何，无需显式深度或变形操作。通过规范化的校正空间和条件引导，模型能够端到端地推断对应关系并填充遮挡区域。为确保公平评估，作者提出了一种端到端的测试协议，避免使用任何真实几何或代理估计，并采用感知舒适度（iSQoE）和几何一致性（MEt3R）等下游相关指标。StereoSpace在分层和非朗伯场景中表现出优异的视差清晰度和鲁棒性，超越了基于变形修复、潜在变形及变形条件等类别的现有方法，证明了视点条件扩散是立体生成的一种可扩展且无需深度的解决方案。",
    "one_sentence": "本文提出了StereoSpace，一种仅通过视点条件建模几何、无需显式深度或变形即可实现单目到立体合成的扩散框架。"
  },
  {
    "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
    "url": "http://arxiv.org/abs/2512.10958v1",
    "abstract_en": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
    "date": "2025-12-11",
    "summary_cn": "AI 接口暂时不可用，请阅读下方英文摘要。",
    "one_sentence": "生成失败"
  },
  {
    "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
    "url": "http://arxiv.org/abs/2512.10957v1",
    "abstract_en": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
    "date": "2025-12-11",
    "summary_cn": "本文提出了SceneMaker，一个解耦的3D场景生成框架。针对现有方法在严重遮挡和开放集场景下难以同时生成高质量几何与精确姿态的问题，该框架首先将去遮挡模型与3D对象生成解耦，并利用图像数据集和收集的去遮挡数据集增强其处理多样化开放集遮挡模式的能力。其次，提出了一个统一姿态估计模型，通过整合全局与局部机制来改进自注意力和交叉注意力，以提高准确性。此外，构建了一个开放集3D场景数据集以扩展姿态估计模型的泛化能力。综合实验表明，该解耦框架在室内和开放集场景中均表现出优越性。",
    "one_sentence": "本文提出了一种解耦的3D场景生成框架SceneMaker，通过分离去遮挡模型与3D对象生成、增强去遮挡多样性、以及结合全局与局部注意力机制的姿态估计模型，有效解决了严重遮挡和开放集场景下高质量几何与精确姿态难以同时生成的问题。"
  },
  {
    "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
    "url": "http://arxiv.org/abs/2512.10956v1",
    "abstract_en": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
    "date": "2025-12-11",
    "summary_cn": "针对现有机器人导航基础模型（NFMs）依赖单目视觉、忽视中层视觉先验而导致数据需求大、在动态非结构化环境中性能受限的问题，本文提出了StereoWalker模型。它通过引入立体视觉输入以解决深度尺度模糊性，并整合了深度估计和密集像素跟踪等显式中层视觉模块，从而更高效地获取几何与运动结构。此外，研究还构建了一个大型立体导航数据集用于训练。实验表明，StereoWalker仅需1.5%的训练数据即可达到现有最佳模型性能，使用全量数据时则能实现超越，且立体视觉相比单目输入显著提升了导航效果。",
    "one_sentence": "本文提出了一种结合立体视觉输入和显式中层视觉模块（如深度估计与密集像素跟踪）的机器人导航基础模型StereoWalker，显著提升了数据效率和导航性能。"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "url": "http://arxiv.org/abs/2512.10955v1",
    "abstract_en": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
    "date": "2025-12-11",
    "summary_cn": "AI 接口暂时不可用，请阅读下方英文摘要。",
    "one_sentence": "生成失败"
  }
]