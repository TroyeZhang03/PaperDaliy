[
  {
    "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs",
    "url": "http://arxiv.org/abs/2601.02360v1",
    "abstract_en": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.",
    "date": "2026-01-05",
    "summary_cn": "针对大语言模型预训练中带宽受限的问题，本文提出了一种异构分布式训练框架。该框架结合了基于稀疏伪梯度交换的SparseLoCo方法与低带宽流水线模型并行技术，通过激活和激活梯度压缩，支持高带宽和资源受限参与者协同训练。实验表明，该方法在178M至1B参数规模下，能有效提升损失与通信的权衡，为低带宽环境下的大模型预训练提供了实用路径。",
    "one_sentence": "本文提出了一种结合SparseLoCo与低带宽流水线模型并行的异构分布式训练框架，通过稀疏伪梯度交换和子空间投影压缩技术，在低带宽环境下高效实现大语言模型预训练。"
  },
  {
    "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "url": "http://arxiv.org/abs/2601.02359v1",
    "abstract_en": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.",
    "date": "2026-01-05",
    "summary_cn": "针对现有深度伪造检测方法在未知操纵上泛化能力差的问题，本文提出了全自监督的ExposeAnyone方法。该方法利用基于扩散模型的音频生成表情序列，通过个性化模型的重建误差计算身份距离，实现了对特定目标的面部伪造检测。实验表明，该方法在多个数据集上性能优异，能有效检测Sora2生成的视频，并对模糊和压缩等干扰具有强鲁棒性。",
    "one_sentence": "本文提出了一种基于扩散模型的全自监督方法ExposeAnyone，通过个性化模型计算重建误差来实现对未知深度伪造的高效检测。"
  },
  {
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "url": "http://arxiv.org/abs/2601.02358v1",
    "abstract_en": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
    "date": "2026-01-05",
    "summary_cn": "VINO是一个统一的视觉生成框架，能够在单一模型中实现图像和视频的生成与编辑。它结合了视觉语言模型和多模态扩散Transformer，通过交错条件标记支持多参考定位和长指令跟随。实验表明，VINO在视觉质量、指令遵循和身份保持方面表现优异，为通用视觉生成提供了实用路径。",
    "one_sentence": "本文提出了VINO，一个通过共享扩散骨干和交错条件标记实现图像与视频统一生成与编辑的视觉生成框架。"
  },
  {
    "title": "DARC: Drum accompaniment generation with fine-grained rhythm control",
    "url": "http://arxiv.org/abs/2601.02357v1",
    "abstract_en": "In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.",
    "date": "2026-01-05",
    "summary_cn": "本文针对现有音乐生成工具在结构控制和风格灵活性上的不足，提出了DARC生成式鼓伴奏模型。该模型能够结合其他音轨的音乐上下文和显式节奏提示（如节拍或敲击音轨），通过参数高效微调技术，在保持音乐上下文感知的同时，实现了对节奏的精细控制。",
    "one_sentence": "本文提出了DARC模型，通过参数高效微调技术，实现了在保持音乐上下文感知的同时，结合其他音轨和显式节奏提示的精细节奏控制。"
  },
  {
    "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
    "url": "http://arxiv.org/abs/2601.02356v1",
    "abstract_en": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
    "date": "2026-01-05",
    "summary_cn": "Talk2Move是一种基于强化学习的扩散框架，旨在解决通过自然语言对场景中物体进行空间变换的难题。该方法利用GRPO探索几何动作，通过空间奖励模型对齐变换与语言描述，并采用离策略评估提高学习效率。实验表明，Talk2Move在空间准确性和场景连贯性上优于现有方法，实现了精确且语义一致的物体变换。",
    "one_sentence": "本文提出了Talk2Move，一种基于强化学习和扩散模型的框架，通过GRPO和对象级空间奖励机制，实现了无需成对数据的文本指导物体几何变换。"
  }
]