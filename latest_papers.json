[
  {
    "title": "Unveiling the 3D structure of the central molecular zone from stellar kinematics and photometry: The 50 and 20 km/s clouds",
    "url": "http://arxiv.org/abs/2601.05252v1",
    "abstract_en": "The central molecular zone (CMZ), surrounding the Galactic centre, is the largest reservoir of dense molecular gas in the Galaxy. Despite its relative proximity, the 3D structure of the CMZ remains poorly constrained, primarily due to projection effects. We aim to constrain the line-of-sight location of two molecular clouds in the CMZ -- the 50 and 20 km/s clouds -- and to investigate their possible physical connection using stellar kinematics and photometry. This study serves as a pilot for future applications across the full CMZ. We estimated the line-of-sight position of the clouds by analysing stellar kinematics, stellar densities, and stellar populations towards the cloud regions and a control field. We find an absence of westward moving stars in the cloud regions, which indicates that they lie on the near side of the CMZ. This interpretation is supported by the stellar density distributions. The similar behaviour observed in the two clouds, as well as in the region between them (the ridge), suggests that they are located at comparable distances and are physically linked. We also identified an intermediate-age stellar population (2-7 Gyr) in both regions, consistent with that observed on the near side of the CMZ. We estimated the line-of-sight distances at which the clouds and the ridge become kinematically detectable (i.e. where the proper motion component parallel to the Galactic plane differs from that of the control field at the 3 sigma level) by converting their measured proper motions parallel to the Galactic plane using a theoretical model of the stellar distribution. We find that the 50 and 20 km/s clouds are located at $43\\pm8$ pc and $56\\pm11$ pc from Sgr A*, respectively, and that the ridge lies at $56\\pm11$ pc; this supports the idea that the clouds are physically connected through the ridge.",
    "date": "2026-01-08",
    "summary_cn": "本研究旨在通过恒星运动学和光度学，确定银河系中心分子区中50和20 km/s云的视线位置及其物理联系。研究发现这些云位于CMZ的近侧，且行为相似，表明它们物理相连。通过理论模型转换测量到的平行于银道面的自行，估算出50 km/s云、20 km/s云和脊分别位于距离Sgr A* 43±8 pc、56±11 pc和56±11 pc处，支持了它们通过脊物理连接的观点。",
    "one_sentence": "本文通过恒星运动学和光度学分析，确定了银河系中心分子区中50和20 km/s云及其连接脊位于近侧，并估算出它们与Sgr A*的距离分别为43±8 pc和56±11 pc，证实它们物理相连。"
  },
  {
    "title": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video",
    "url": "http://arxiv.org/abs/2601.05251v1",
    "abstract_en": "We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.",
    "date": "2026-01-08",
    "summary_cn": "Mesh4D是一种单目4D网格重建的前馈模型，能够从视频中重建物体的完整3D形状和运动。其核心创新在于通过自编码器学习紧凑潜在空间，利用骨骼结构作为训练先验，但在推理时无需骨骼信息。模型采用时空注意力机制，并结合潜在扩散模型，一次性预测完整动画序列，在重建和新视角合成任务中表现优异。",
    "one_sentence": "本文提出Mesh4D，一种通过紧凑潜在空间和时空注意力机制，实现单目视频动态物体4D网格重建的前馈模型。"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "url": "http://arxiv.org/abs/2601.05249v1",
    "abstract_en": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
    "date": "2026-01-08",
    "summary_cn": "夜间色彩恒常性因低光噪声和复杂光照条件而极具挑战性。本文提出RL-AWB框架，融合统计方法与深度强化学习，通过显著性灰像素检测和光照估计实现夜间白平衡。该方法模拟专家调优过程，动态优化参数，并引入首个多传感器夜间数据集。实验表明，该方法在低光和正常光照图像中均表现出优异的泛化能力。",
    "one_sentence": "本文提出了一种结合统计方法与深度强化学习的RL-AWB框架，通过动态优化参数实现夜间白平衡，并引入首个多传感器夜间数据集以提升跨传感器泛化能力。"
  },
  {
    "title": "QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer",
    "url": "http://arxiv.org/abs/2601.05250v1",
    "abstract_en": "Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.",
    "date": "2026-01-08",
    "summary_cn": "针对经典NeRF模型参数量大、训练密集的问题，本文提出了QNeRF，首个用于新视图合成的混合量子-经典模型。QNeRF利用量子电路的叠加和纠缠特性编码空间与视角信息，显著降低了模型参数量。实验表明，在中等分辨率图像上，QNeRF在参数量不到经典模型一半的情况下，性能仍能匹配或超越基线模型，为计算机视觉中的连续信号表示提供了有效的量子替代方案。",
    "one_sentence": "本文提出了QNeRF，首个用于从2D图像进行新视图合成的混合量子-经典模型，通过参数化量子电路编码空间和视角信息，实现了比经典NeRF更紧凑的模型表示。"
  },
  {
    "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model",
    "url": "http://arxiv.org/abs/2601.05248v1",
    "abstract_en": "Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0",
    "date": "2026-01-08",
    "summary_cn": "针对现有视觉-语言-动作模型显式推理带来的延迟和表达瓶颈问题，本文提出了LaST0框架。该方法利用潜在时空思维链捕捉难以言喻的物理属性，并通过混合Transformer双系统架构分离低频推理与高频动作生成。实验表明，LaST0在模拟和真实任务中均显著提升了成功率，且推理速度更快。",
    "one_sentence": "本文提出LaST0框架，通过潜在时空思维链和双系统架构实现高效的隐式推理，在提升机器人操作成功率的同时显著降低推理延迟。"
  }
]