[
  {
    "title": "RedSage: A Cybersecurity Generalist LLM",
    "url": "http://arxiv.org/abs/2601.22159v1",
    "abstract_en": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
    "date": "2026-01-29",
    "summary_cn": "该研究旨在构建一个无需依赖外部API、可本地部署的网络安全大语言模型助手。为此，团队收集并整理了大规模的网络安全领域预训练数据，并通过模拟专家工作流生成了高质量微调数据集。基于这些数据训练出的RedSage模型，在自建的RedSage-Bench及多个现有网络安全基准测试和通用任务上，均取得了超过基线模型的显著性能提升。所有模型、数据和代码均已开源。",
    "one_sentence": "本文提出并训练了一个名为RedSage的开源本地化网络安全大语言模型，其通过领域感知的持续预训练和基于智能体仿真的微调数据增强，在网络安全和通用任务上均显著优于基线模型。"
  },
  {
    "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
    "url": "http://arxiv.org/abs/2601.22158v1",
    "abstract_en": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
    "date": "2026-01-29",
    "summary_cn": "现代扩散/流模型通常依赖于多步采样和潜空间操作。本研究提出“像素平均流”（pMF），其核心思想是分别构建网络输出空间和损失空间：网络输出目标被设计在预设的低维图像流形上，而损失则在速度空间通过平均流定义，并引入了图像流形与平均速度场之间的简单变换。实验表明，pMF在ImageNet数据集上，以256x256和512x512分辨率分别取得了2.22和2.48的FID得分，在不使用潜空间的一步生成领域取得了关键进展，有望进一步拓展基于扩散/流的生成模型边界。",
    "one_sentence": "本文提出了一种名为‘像素平均流’（pMF）的方法，通过将网络输出空间与损失空间分别构建，实现了不依赖潜空间的一步式图像生成。"
  },
  {
    "title": "Discovering Hidden Gems in Model Repositories",
    "url": "http://arxiv.org/abs/2601.22157v1",
    "abstract_en": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.",
    "date": "2026-01-29",
    "summary_cn": "研究发现公共模型仓库中，性能显著优于主流模型的“隐藏宝石”模型普遍存在，但被社区忽略。为了从海量模型中高效发现此类优质模型，作者将问题构建为多臂老虎机任务，并提出一种加速的序列减半搜索方法，通过共享查询和激进淘汰，能以每个候选模型仅约50次查询的代价找到顶尖模型，将发现速度提升了50倍以上。",
    "one_sentence": "本文提出了一种将大语言模型社区中的“隐藏宝石”模型发现问题建模为多臂老虎机问题，并利用共享查询集和激进淘汰策略加速序列减半搜索算法，从而高效发掘性能优越但未被广泛使用的模型的方法。"
  },
  {
    "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
    "url": "http://arxiv.org/abs/2601.22156v1",
    "abstract_en": "Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data",
    "date": "2026-01-29",
    "summary_cn": "现有的将预训练Transformer转化为软注意力与循环神经网络（RNN）混合模型的方法存在训练数据需求大、长上下文性能差的缺点。本文提出HALO转化流程，仅需极少量数据即可完成转化；并提出HypeNet混合架构，通过新的位置编码方案HyPE和其他改进，获得了优异的长上下文泛化能力。将Qwen3系列模型转化为HypeNet后，在保持与原模型相当性能的同时，显著提升了长上下文处理效率和性能。",
    "one_sentence": "本文提出了一种名为HALO的高效转化方法和一种名为HypeNet的新型混合架构，能够以极少量数据将预训练Transformer模型转化为具有优越长上下文性能和效率的混合模型。"
  },
  {
    "title": "UEval: A Benchmark for Unified Multimodal Generation",
    "url": "http://arxiv.org/abs/2601.22155v1",
    "abstract_en": "We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.",
    "date": "2026-01-29",
    "summary_cn": "本文介绍了UEval基准测试，用于评估能够同时生成图像和文本的统一模型。该基准包含1000个来自真实场景的专家精心设计问题，要求模型输出图文结合的回答。为解决开放性多模态生成的评估难题，研究团队设计了一套基于评分准则的系统：首先由多模态大语言模型根据参考答案生成初步准则，再由专家验证和细化，最终形成了超过一万个验证过的评分标准。实验表明，当前统一模型在UEval上表现不佳，最佳模型得分仅66.4。研究发现推理能力对复杂多模态任务至关重要，将推理链从具备推理能力的模型迁移到非推理模型能显著缩小性能差距。",
    "one_sentence": "本文提出了UEval基准测试，通过专家设计并验证的细粒度评分准则来评估能够同时生成图像和文本的统一模型。"
  }
]