[
  {
    "title": "Reinforced Attention Learning",
    "url": "http://arxiv.org/abs/2602.04884v1",
    "abstract_en": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.   We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.",
    "date": "2026-02-04",
    "summary_cn": "研究发现，传统的基于输出理由的强化学习后训练方法对提升多模态大语言模型的感知能力效果有限，有时甚至会损害性能。为此，本文提出强化注意力学习框架，将优化目标从‘生成什么’转向‘关注哪里’，直接调整模型内部跨模态注意力分布。该方法在多个图像和视频基准测试中表现优于现有基线。此外，作者还引入了基于策略的注意力蒸馏技术，证明传递潜在的注意力行为比标准的知识蒸馏能带来更好的跨模态对齐效果。这项研究为多模态后训练提供了一种新的理论性替代方案。",
    "one_sentence": "本文提出了一个名为‘强化注意力学习’的策略梯度框架，通过直接优化多模态大语言模型内部的注意力分布，而非输出文本序列，来提升模型对复杂多模态信息的处理和推理能力。"
  },
  {
    "title": "Protein Autoregressive Modeling via Multiscale Structure Generation",
    "url": "http://arxiv.org/abs/2602.04883v1",
    "abstract_en": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.",
    "date": "2026-02-04",
    "summary_cn": "本文提出了蛋白质自回归模型，这是一种创新的多尺度自回归框架，用于从粗到细生成蛋白质骨架结构。该方法利用蛋白质的层次特性，通过多尺度下采样、自回归Transformer编码和多条件嵌入指导，以及基于流的解码器，逐步精修结构。为应对自回归模型的曝光偏差问题，模型采用了噪声上下文学习和计划采样策略，显著提升了生成质量。PAR展现出了强大的零样本泛化能力，支持灵活的条件生成和基序支架，在无条件生成基准测试中表现出色，是蛋白质结构生成的有前景框架。",
    "one_sentence": "本文提出了首个多尺度自回归蛋白质骨架生成框架，通过逐级粗化到细化的方式模拟雕塑过程，并有效缓解了自回归模型中的曝光偏差问题。"
  },
  {
    "title": "Pairs of differential forms: a framework for precontact geometry",
    "url": "http://arxiv.org/abs/2602.04882v1",
    "abstract_en": "Precontact manifolds extend contact geometry by weakening the maximal non-integrability condition of the defining $1$-form. We clarify the geometric foundations of this structure by studying general pairs of a $1$-form and a $2$-form under mild regularity conditions. We characterize them through their class, analyse the role of distinguished vector fields, such as Reeb or Liouville fields, and study other associated geometrical objects. Precontact structures are then treated as the special case of pairs formed by a nowhere-vanishing $1$-form and its exterior derivative. We also define Hamiltonian dynamics on precontact manifolds. Several examples are presented to illustrate the theory.",
    "date": "2026-02-04",
    "summary_cn": "本文以放宽定义1-形式的极大非可积条件为出发点，系统研究了由满足温和正则条件的一形式与二形式对所构成的几何结构。作者通过其类对这些结构进行刻画，分析了诸如Reeb场或Liouville场等特殊向量场的作用，并探讨了其他相关的几何对象。预接触结构被处理为不可为零1-形式及其外微分构成的特殊情况。文章还在预接触流形上定义了哈密顿动力学，并通过多个示例对理论进行了阐释。",
    "one_sentence": "本文通过系统研究满足特定正则条件的一形式与二形式对所构成的几何结构，澄清了预接触流形的几何基础，并将其推广至包含哈密顿动力学的一般理论框架。"
  },
  {
    "title": "Contrastive Continual Learning for Model Adaptability in Internet of Things",
    "url": "http://arxiv.org/abs/2602.04881v1",
    "abstract_en": "Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \\emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.",
    "date": "2026-02-04",
    "summary_cn": "物联网应用常面临非平稳动态环境、资源约束和隐私需求等挑战。本文回顾了结合对比学习和持续学习的对比持续学习方法，该方法旨在提升模型鲁棒性并避免灾难性遗忘。文章连接了算法设计与物联网现实约束，提出了一个统一的参考架构和评估方案，并指出了数据异构、概念漂移、联邦学习和能耗感知等未来挑战。",
    "one_sentence": "本文系统性地回顾并提出了将对比学习与持续学习相结合的对比持续学习方法，以应对物联网中数据动态、资源受限等挑战，并提出了一个面向物联网的参考架构与评估指导。"
  },
  {
    "title": "Capturing Visual Environment Structure Correlates with Control Performance",
    "url": "http://arxiv.org/abs/2602.04880v1",
    "abstract_en": "The choice of visual representation is key to scaling generalist robot policies. However, direct evaluation via policy rollouts is expensive, even in simulation. Existing proxy metrics focus on the representation's capacity to capture narrow aspects of the visual world, like object shape, limiting generalization across environments. In this paper, we take an analytical perspective: we probe pretrained visual encoders by measuring how well they support decoding of environment state -- including geometry, object structure, and physical attributes -- from images. Leveraging simulation environments with access to ground-truth state, we show that this probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, significantly outperforming prior metrics and enabling efficient representation selection. More broadly, our study provides insight into the representational properties that support generalizable manipulation, suggesting that learning to encode the latent physical state of the environment is a promising objective for control.",
    "date": "2026-02-04",
    "summary_cn": "提升通用机器人策略的关键在于选择合适的视觉表征，但直接评估成本高昂。本文提出一种分析视角：通过探究预训练视觉编码器从图像中解码环境真实状态（如几何、物体结构、物理属性）的准确度，来衡量其性能。研究发现，这种探测准确性与下游策略在不同环境和学习设置下的表现强相关，显著优于现有指标，能有效指导表征选择，并为实现通用机器人操作提供了表征层面的新见解。",
    "one_sentence": "本文提出了一种通过衡量视觉编码器解码环境状态的能力来评估其性能的代理指标，该指标与下游策略性能强相关，能高效选择通用机器人策略的视觉表征。"
  }
]