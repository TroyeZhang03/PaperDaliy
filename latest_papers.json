[
  {
    "title": "EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing",
    "url": "http://arxiv.org/abs/2602.15031v1",
    "abstract_en": "High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.",
    "date": "2026-02-16",
    "summary_cn": "本文介绍了EditCtrl，一种高效的视频修复控制框架，通过仅关注掩码区域显著降低计算成本。该方法引入局部视频上下文模块和轻量级时间全局上下文嵌入器，确保局部优先生成并保持全局一致性。实验表明，EditCtrl不仅比现有方法计算效率提高10倍，还在编辑质量上有所提升，支持多区域编辑和自回归内容传播等新功能。",
    "one_sentence": "本文提出了EditCtrl框架，通过引入局部视频上下文模块和轻量级时间全局上下文嵌入器，仅在被掩码覆盖的区域进行计算，从而显著提高了视频修复的计算效率。"
  },
  {
    "title": "Image Generation with a Sphere Encoder",
    "url": "http://arxiv.org/abs/2602.15030v1",
    "abstract_en": "We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .",
    "date": "2026-02-16",
    "summary_cn": "本文提出了Sphere Encoder，一种高效的生成框架，通过学习将自然图像均匀映射到球形潜在空间，并利用解码器将随机潜在向量映射回图像空间。该模型仅需少于五步即可生成高质量图像，性能媲美多步扩散模型，且推理成本大幅降低。架构支持条件生成，并通过编码器/解码器的多次循环进一步提升图像质量。实验表明，该方法在多个数据集上表现优异，兼具高效性与竞争力。",
    "one_sentence": "本文提出了Sphere Encoder，一种通过学习将自然图像均匀映射到球形潜在空间，并利用解码器在极少步数内生成高质量图像的高效生成框架。"
  },
  {
    "title": "Symmetry in language statistics shapes the geometry of model representations",
    "url": "http://arxiv.org/abs/2602.15029v1",
    "abstract_en": "Although learned representations underlie neural networks' success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.",
    "date": "2026-02-16",
    "summary_cn": "尽管神经网络的成功依赖于学习到的表示，但其基本特性仍不明确。本文发现语言统计中的平移对称性（如月份共现概率仅取决于时间间隔）决定了高维词嵌入模型中的几何结构。研究还表明，即使在共现统计受到强烈干扰时，这些结构仍具有鲁棒性，这源于潜在连续变量的集体控制。该理论框架在词嵌入、文本嵌入和大语言模型中得到了验证。",
    "one_sentence": "本文证明了语言统计中的平移对称性导致了高维词嵌入模型中几何结构的涌现，并揭示了这些结构在统计扰动下的鲁棒性源于潜在连续变量的集体控制。"
  },
  {
    "title": "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization",
    "url": "http://arxiv.org/abs/2602.15028v1",
    "abstract_en": "Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench",
    "date": "2026-02-16",
    "summary_cn": "本文提出大规模基准PAPerBench，系统研究上下文长度对大模型个性化与隐私的影响。实验发现随上下文增长，模型在个性化和隐私保护上均表现退化。研究通过理论分析指出这是固定容量Transformer中注意力稀释的固有局限，揭示了当前模型存在的“长上下文、少聚焦”的扩展差距。",
    "one_sentence": "本文提出了PAPerBench基准，首次系统揭示了上下文长度增加会导致大模型个性化质量下降与隐私泄露加剧，并通过理论分析指出这是注意力稀释的固有限制。"
  },
  {
    "title": "Complementarity of di-top and four-top searches in interpreting possible signals of new physics",
    "url": "http://arxiv.org/abs/2602.15027v1",
    "abstract_en": "Final states comprising two or more top quarks are important search channels at the Large Hadron Collider for scalar particles predicted in models of physics beyond the Standard Model. While the di-top final state profits from a higher signal cross section, it can be subject to intricate interference patterns. Besides the interference with the large QCD background, in case of the presence of more than one high-mass scalar also large signal--signal interference contributions can occur. We show that in such scenarios it is crucial to account for loop-level mixing for obtaining accurate exclusion bounds. We demonstrate how the interference patterns can obscure the interpretation of possible deviations from the Standard Model expectations. We show that the four-top final state, while giving rise to a smaller signal cross section, provides important complementary information due to its much smaller signal--background interference contributions. Thus, the results obtained from the four-top final state can be instrumental for pinpointing the underlying new physics scenario.",
    "date": "2026-02-16",
    "summary_cn": "本文研究了大型强子对撞机上双顶夸克和四顶夸克末态对标量粒子的寻找。研究发现，双顶夸克末态中信号间及信号-背景间的复杂干涉效应会影响对超越标准模型偏差的解释。作者指出，在存在多个高质量标量的场景中，圈层混合效应对于获得准确的排除界限至关重要。相比之下，四顶夸克末态虽然截面较小，但受信号-背景干涉影响小，能提供重要的互补信息，有助于确定潜在的新物理场景。",
    "one_sentence": "本文指出在多标量粒子模型中，考虑圈层混合效应对于获得准确的排除界限至关重要。"
  }
]