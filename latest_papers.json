[
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "url": "http://arxiv.org/abs/2512.10959v1",
    "abstract_en": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
    "date": "2025-12-11",
    "summary_cn": "该论文介绍了StereoSpace，一个基于扩散模型的单目转立体图像合成框架。其核心创新在于仅依赖视角条件来隐式建模几何关系，而无需使用显式的深度信息或图像扭曲操作。该方法通过一个规范的校正空间和条件引导，端到端地推断像素对应关系并填充因视角变化产生的遮挡区域。为确保公平评估，作者提出了一个端到端的测试协议，并采用与下游应用相关的感知舒适度和几何一致性指标。实验表明，StereoSpace在复杂场景下超越了多种现有方法，证明了基于视角条件的扩散模型是解决立体生成问题的一个可扩展且无需深度的有效方案。",
    "one_sentence": "本文提出了一种名为StereoSpace的、仅通过视角条件建模几何的单目到立体合成扩散框架，无需显式深度或图像扭曲。"
  },
  {
    "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
    "url": "http://arxiv.org/abs/2512.10958v1",
    "abstract_en": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
    "date": "2025-12-11",
    "summary_cn": "生成式世界模型能创建逼真的4D驾驶环境，但常存在物理或行为缺陷，且缺乏统一评估标准。为此，本文提出了WorldLens基准，从生成、重建、行动跟随、下游任务和人类偏好五个维度综合评估世界模型。研究还构建了带有人工标注的大规模数据集WorldLens-26K，并开发了自动评估代理WorldLens-Agent，形成了一个衡量世界真实性的统一生态系统，旨在标准化未来模型的评估，不仅关注视觉逼真度，更注重其行为真实性。",
    "one_sentence": "本文提出了一个名为WorldLens的全方位基准测试，包含数据集和评估代理，用于统一评估生成式世界模型的几何、物理和行为真实性。"
  },
  {
    "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
    "url": "http://arxiv.org/abs/2512.10957v1",
    "abstract_en": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
    "date": "2025-12-11",
    "summary_cn": "针对现有方法在严重遮挡和开放集环境下难以同时生成高质量几何与精确位姿的问题，本文提出了SceneMaker框架。该框架首先将去遮挡模型与3D物体生成解耦，并利用图像和专用数据集增强其对多样遮挡的处理能力；其次，设计了一个融合全局与局部注意力的统一位姿估计模型以提高精度，并构建了开放集3D场景数据集来增强模型泛化能力。实验证明，该方法在室内及开放集场景中均表现优异。",
    "one_sentence": "本文提出了一种名为SceneMaker的解耦式3D场景生成框架，通过分离去遮挡模型与3D物体生成、并采用结合全局与局部注意力的统一位姿估计模型，有效提升了复杂遮挡和开放集场景下的生成质量。"
  },
  {
    "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
    "url": "http://arxiv.org/abs/2512.10956v1",
    "abstract_en": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
    "date": "2025-12-11",
    "summary_cn": "针对当前端到端机器人导航基础模型（NFMs）依赖单目视觉而忽略中层视觉模块导致训练数据需求大、在动态非结构化环境中性能受限的问题，本文提出了StereoWalker模型。该模型利用立体视觉输入消除深度模糊，并整合了深度估计、像素跟踪等显式中层视觉线索。研究还构建了一个大型立体导航数据集用于训练。实验表明，StereoWalker仅需1.5%的训练数据即可达到与现有最佳模型相当的性能，并在全量数据下实现超越，同时验证了立体视觉相较于单目输入的优越性。",
    "one_sentence": "本文提出StereoWalker，通过引入立体视觉输入和显式中层视觉信息（如深度估计和密集像素跟踪）来增强机器人导航基础模型，有效解决了单目视觉的深度尺度模糊问题并提升了导航性能。"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "url": "http://arxiv.org/abs/2512.10955v1",
    "abstract_en": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
    "date": "2025-12-11",
    "summary_cn": "针对现有视觉概念个性化方法因使用通用图像编码器而导致属性纠缠和信息泄露的问题，本文提出了Omni-Attribute编码器。该方法通过构建带有正负属性标注的语义关联图像对数据，并结合生成保真度与对比解纠缠的双目标训练范式，学习分离度高、保真度强的属性表示。实验表明，该方法在开放词汇属性检索、个性化和组合生成任务上均达到了领先水平。",
    "one_sentence": "本文提出了首个开放词汇的图像属性编码器Omni-Attribute，通过联合设计数据和模型，学习高保真且属性特定的表示，以解决现有方法中视觉因素纠缠的问题。"
  }
]