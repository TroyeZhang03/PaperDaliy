[
  {
    "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
    "url": "http://arxiv.org/abs/2512.16923v1",
    "abstract_en": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
    "date": "2025-12-18",
    "summary_cn": "针对单图像重对焦的难题，本文提出了“生成式重对焦”方法。该方法分为两步：首先使用DeblurNet从模糊输入中恢复全焦图像，再利用BokehNet生成可控的散景效果。其核心创新在于半监督训练策略，通过结合模拟器生成的配对数据和带有EXIF信息的真实散焦图像，有效学习真实光学特性，克服了现有方法依赖全焦输入或纯合成数据的局限。实验表明，该方法在多项任务上表现优异，并支持文本引导调整和自定义光圈形状。",
    "one_sentence": "本文提出了一种结合合成配对数据与真实非配对散焦图像的半监督训练方法，用于生成可控的单图像重对焦效果。"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "url": "http://arxiv.org/abs/2512.16924v1",
    "abstract_en": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
    "date": "2025-12-18",
    "summary_cn": "该论文介绍了WorldCanvas框架，它突破了仅依赖文本或轨迹的现有方法，将编码了运动、时序和可见性的轨迹、表达语义意图的自然语言以及提供视觉基础的参考图像相结合。这种多模态方法能够生成包含多智能体交互、物体出入、参考引导的外观以及反直觉事件的连贯可控视频。生成的视频不仅时间连贯，还能在物体暂时消失后保持其身份和场景的一致性，从而将世界模型从被动预测工具提升为可由用户交互塑造的模拟器。",
    "one_sentence": "本文提出了WorldCanvas框架，通过结合文本、运动轨迹和参考图像，实现了可由用户引导的、可控且连贯的丰富世界事件生成。"
  },
  {
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "url": "http://arxiv.org/abs/2512.16922v1",
    "abstract_en": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.",
    "date": "2025-12-18",
    "summary_cn": "受自然语言生成式预训练成功的启发，本研究探索了一种新的视觉自监督学习范式NEPA。该方法摒弃了传统的像素重建或对比学习，转而训练模型以自回归方式预测未来的图像块嵌入。实验表明，仅以此为目标在ImageNet-1K上预训练的Transformer模型，无需复杂设计，即可在图像分类和语义分割等多个任务上取得优异性能，为视觉学习提供了一种简单、可扩展的替代方案。",
    "one_sentence": "本文提出了一种名为NEPA的自监督视觉学习方法，通过让模型直接预测未来的图像块嵌入来学习，而非生成用于下游任务的特征表示。"
  },
  {
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "url": "http://arxiv.org/abs/2512.16920v1",
    "abstract_en": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
    "date": "2025-12-18",
    "summary_cn": "本文针对视频编辑在一致性、控制和泛化性方面的挑战，提出了EasyV2V框架。该框架从数据、模型和控制三方面进行创新：通过组合专家方法构建多样化视频数据对，并利用伪标签和过渡监督进行训练；发现预训练文生视频模型已具备编辑能力，从而采用简单的序列拼接和轻量级LoRA微调来构建模型；通过单一掩码机制统一时空控制并支持参考图像。EasyV2V支持多种输入方式，在视频编辑效果上超越了现有系统。",
    "one_sentence": "本文提出了EasyV2V框架，通过数据构建、简化模型设计和统一控制机制，实现了高效且效果卓越的指令式视频编辑。"
  },
  {
    "title": "DVGT: Driving Visual Geometry Transformer",
    "url": "http://arxiv.org/abs/2512.16919v1",
    "abstract_en": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.",
    "date": "2025-12-18",
    "summary_cn": "感知和重建3D场景几何对自动驾驶至关重要。为克服现有方法依赖精确相机参数、难以适应不同场景的局限，本文提出DVGT模型。它采用DINO骨干网络提取特征，并结合局部、空间和时序注意力机制，从无位姿的多视角图像序列中直接推理出度量尺度的全局3D点云地图和自车位姿。该模型无需3D几何先验或与外部传感器对齐，在多个大型驾驶数据集上训练后，性能显著优于现有方法。",
    "one_sentence": "本文提出了一种无需精确相机参数和显式3D几何先验的 Driving Visual Geometry Transformer (DVGT) 模型，能够从任意配置的多视角图像序列中直接感知和重建全局稠密3D场景几何。"
  }
]