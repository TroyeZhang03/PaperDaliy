[
  {
    "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance",
    "url": "http://arxiv.org/abs/2512.11800v1",
    "abstract_en": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.",
    "date": "2025-12-12",
    "summary_cn": "3D高斯溅射（3DGS）虽能实现快速优化和实时渲染，但其简化的alpha混合和密度积分近似限制了复杂半透明物体的渲染效果。本文提出了一种基于统计矩的透射率计算方法，通过分析计算每个像素的统计矩，重建连续透射率函数，并在各高斯内独立采样，从而在不依赖光线追踪或逐像素排序的情况下，精确模拟复杂半透明介质中的光衰减，显著提升了重建与渲染质量。",
    "one_sentence": "本文提出了一种基于统计矩的高精度透射率计算方法，无需光线追踪或逐像素排序，显著提升了3D高斯溅射在复杂半透明物体渲染中的物理精度和渲染质量。"
  },
  {
    "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
    "url": "http://arxiv.org/abs/2512.11799v1",
    "abstract_en": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
    "date": "2025-12-12",
    "summary_cn": "大规模视频生成模型在建模真实场景外观和光照交互方面展现出巨大潜力，但缺乏一个能够联合理解内禀场景属性（如反照率、法线、材质和辐照度）、利用它们进行视频合成并支持可编辑内禀表示的闭环框架。本文提出了V-RGBX，这是首个端到端的内禀感知视频编辑框架。V-RGBX统一了视频逆向渲染、基于内禀表示的光照真实视频合成和基于关键帧的视频编辑三项关键能力。其核心是一种交织条件机制，允许用户通过选择关键帧进行直观、物理基础的视频编辑，并支持灵活操纵任何内禀模态。广泛的定性和定量结果表明，V-RGBX能够生成时间一致、光照真实的视频，并以物理合理的方式将关键帧编辑传播到整个序列。该框架在物体外观编辑和场景级重光照等多种应用中表现出色，超越了现有方法。",
    "one_sentence": "本文提出了首个端到端的内禀感知视频编辑框架V-RGBX，它统一了视频逆向渲染、基于内禀表示的视频合成和基于关键帧的视频编辑三项关键能力，通过交织条件机制实现了直观、物理真实的视频编辑。"
  },
  {
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "url": "http://arxiv.org/abs/2512.11798v1",
    "abstract_en": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.",
    "date": "2025-12-12",
    "summary_cn": "本文介绍了Particulate，这是一种前馈方法，通过核心的Part Articulation Transformer网络处理输入网格的点云，直接预测日常物体底层铰接结构的3D部件、运动学结构和运动约束。该方法在公共数据集上端到端训练，推理时可在数秒内将预测结果提升至输入网格，生成完全铰接的3D模型，速度快于以往需要逐对象优化的方法。它还能准确推断AI生成的3D资产的铰接结构，结合现成的图像到3D生成器，可从单张图像中提取完整的铰接3D对象。研究还引入了新的铰接估计基准并改进了评估协议，实验表明其性能显著优于现有技术。",
    "one_sentence": "本文提出了一种名为Particulate的前馈式方法，能够直接从单个静态3D网格中推断出底层铰接结构的所有属性，包括3D部件、运动学结构和运动约束。"
  },
  {
    "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
    "url": "http://arxiv.org/abs/2512.11797v1",
    "abstract_en": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.",
    "date": "2025-12-12",
    "summary_cn": "针对模仿学习中大规模多样化机器人演示数据难以获取的问题，本文提出了AnchorDream方法。该方法基于预训练的视频扩散模型，通过以机器人运动渲染为条件来锚定其具身性，防止生成不合理动作，同时合成与机器人运动学一致的对象和环境。仅需少量人类遥操作演示，即可扩展为大规模、高质量的数据集，无需显式环境建模。实验表明，生成的数据能显著提升下游策略学习性能，在仿真和真实世界任务中分别获得36.4%和近两倍的性能提升，为扩展模仿学习提供了可行路径。",
    "one_sentence": "本文提出了一种名为AnchorDream的具身感知世界模型，通过将预训练视频扩散模型重新用于机器人数据合成，并利用机器人运动渲染来锚定具身性，从而能够从少量演示中生成大规模、多样化且高质量的数据集。"
  },
  {
    "title": "Loop-string-hadron approach to SU(3) lattice Yang-Mills theory, II: Operator representation for the trivalent vertex",
    "url": "http://arxiv.org/abs/2512.11796v1",
    "abstract_en": "This work is the second installment of a series on the Loop-String-Hadron (LSH) approach to SU(3) lattice Yang-Mills theory. Here, we present the infinite-dimensional matrix representation for arbitrary gauge-invariant operators at a trivalent vertex, which results in a standalone framework for computations that supersedes the underlying Schwinger-boson framework. To that end, we evaluate in closed form the result of applying any gauge-invariant operators on the LSH basis states introduced in Part I. Classical calculations in the LSH basis run significantly faster than equivalent calculations performed using Schwinger bosons. A companion code script is provided, which implements the derived formulas and aims to facilitate rapid progress towards Hamiltonian-based calculations of quantum chromodynamics.",
    "date": "2025-12-12",
    "summary_cn": "AI 接口暂时不可用，请阅读下方英文摘要。",
    "one_sentence": "生成失败"
  }
]