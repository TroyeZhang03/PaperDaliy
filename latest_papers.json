[
  {
    "title": "Dataset Distillation for Pre-Trained Self-Supervised Vision Models",
    "url": "http://arxiv.org/abs/2511.16674v1",
    "abstract_en": "The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.",
    "date": "2025-11-20",
    "summary_cn": "传统数据集蒸馏方法旨在为随机初始化的模型合成训练数据，而本文关注如何为大型预训练视觉模型（如CLIP、DINO）蒸馏出能高效训练线性分类器的数据集。作者提出的线性梯度匹配法，通过优化合成图像，使其在预训练特征提取器中产生的线性分类器梯度与真实数据一致。该方法生成的合成数据性能优于真实图像基线，并能跨模型通用，在细粒度分类和模型可解释性方面表现出色。",
    "one_sentence": "本文提出了一种名为线性梯度匹配的数据集蒸馏方法，专门针对预训练模型优化合成数据，以高效训练线性分类器。"
  },
  {
    "title": "EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards",
    "url": "http://arxiv.org/abs/2511.16672v1",
    "abstract_en": "Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\\sim$3\\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.",
    "date": "2025-11-20",
    "summary_cn": "针对现有大型多模态模型训练依赖人工标注数据或外部奖励模型的问题，本研究提出了纯无监督的自进化框架EvoLMM。该框架从单一骨干模型实例化出两个协作智能体：提议者生成多样化的图像相关问题，求解者通过内部一致性进行解答，并通过持续的自奖励过程进行学习。实验表明，以Qwen2.5-VL为基础模型的EvoLMM在多个数学推理基准测试上取得了显著提升，为完全无监督的自改进多模态模型研究提供了有效基线。",
    "one_sentence": "本文提出了一种名为EvoLMM的自进化框架，通过在同一骨干模型中构建提议者和求解者两个协作智能体，实现了无需人工标注或奖励模型的纯无监督多模态大模型自我提升。"
  },
  {
    "title": "NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses",
    "url": "http://arxiv.org/abs/2511.16673v1",
    "abstract_en": "We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate \"ground-truth\" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).",
    "date": "2025-11-20",
    "summary_cn": "本文针对从单张或稀疏图像重建可动画3D人体化身的任务，指出现有先进方法在测试时依赖精确的相机和人体姿态作为输入，若姿态估计不准会严重影响重建效果。为此，作者提出了NoPo-Avatar方法，它仅依据图像进行重建，完全不需要姿态输入。这种设计使其不受噪声姿态估计的影响，应用更广泛。在多个挑战性数据集上的实验表明，该方法在无真实姿态的实用场景下优于现有基线，在有真实姿态的实验室环境下也能取得相当的结果。",
    "one_sentence": "本文提出了一种名为NoPo-Avatar的新方法，无需任何人体姿态输入即可从单张或稀疏图像重建可动画的3D人体化身，从而克服了现有方法对噪声姿态估计的依赖。"
  },
  {
    "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
    "url": "http://arxiv.org/abs/2511.16671v1",
    "abstract_en": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
    "date": "2025-11-20",
    "summary_cn": "现有视觉生成方法通常在生成前或生成后引入文本推理，但缺乏生成过程中的实时交互。本研究提出了Thinking-while-Generating (TwiG)框架，首次实现了在视觉内容逐步生成的同时交织进行文本推理，既能指导后续局部区域的生成，也能反思已合成的部分。这种动态交互能产生更具上下文感知和语义丰富的视觉输出。为探索该框架潜力，作者研究了零样本提示、有监督微调和强化学习三种策略，并构建了TwiG-50K数据集。这项工作为融合文本推理以增强视觉生成提供了新的研究方向。",
    "one_sentence": "本文提出了首个在视觉生成过程中交织进行文本推理的框架TwiG，实现了文本推理与图像生成的动态协同演化。"
  },
  {
    "title": "Learning to Think Fast and Slow for Visual Language Models",
    "url": "http://arxiv.org/abs/2511.16670v1",
    "abstract_en": "When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.",
    "date": "2025-11-20",
    "summary_cn": "现有视觉语言模型在处理问题时通常采用冗长的推理链，导致计算成本高昂。受人类快慢思维机制启发，本文提出DualMindVLM模型。该方法首先根据模型输出长度标注数据所需的思维模式，然后利用GRPO进行训练，使模型能依据任务难度自动选择快速或慢速思考。实验表明，该方法在保持高性能的同时，显著提升了计算效率。",
    "one_sentence": "本文提出了一种基于强化学习的双模式思维视觉语言模型，可根据问题难度自动切换快速与慢速思考模式，以提升推理效率。"
  }
]