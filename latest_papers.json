[
  {
    "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "url": "http://arxiv.org/abs/2512.17909v1",
    "abstract_en": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.",
    "date": "2025-12-19",
    "summary_cn": "当前基于表征编码器高维特征的生成模型面临潜在空间缺乏规整和像素重建能力弱两大挑战。为此，本文提出一个系统性框架，通过引入语义-像素联合重建目标，将丰富的语义信息和细粒度细节压缩到一个高度紧凑的潜在表示中。该方法不仅实现了最先进的图像重建质量，还显著提升了文本到图像生成和图像编辑任务的性能与收敛速度，证明了理解型编码器可被有效改造为强大的生成组件。",
    "one_sentence": "本文提出了一种通过引入语义-像素重建目标来规整表征编码器特征的方法，使其能同时压缩语义信息和细节，从而构建一个适用于生成任务的紧凑且高性能的潜在空间。"
  },
  {
    "title": "Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting",
    "url": "http://arxiv.org/abs/2512.17908v1",
    "abstract_en": "Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.",
    "date": "2025-12-19",
    "summary_cn": "针对Depth Anything V2等基础模型在处理分布外真实图像时性能下降的问题，本文提出了Re-Depth Anything测试时自监督框架。该方法通过融合大规模2D扩散模型的先验知识，无需真实标签即可优化深度图。核心创新在于利用生成式场景下的从阴影恢复形状（SfS）线索和分数蒸馏采样（SDS）进行图像重合成，并采用冻结编码器、仅优化嵌入和微调解码器的策略以避免模型崩溃。实验表明，该方法在多个基准测试中显著提升了深度估计的准确性和真实感。",
    "one_sentence": "本文提出了Re-Depth Anything，一种在测试时利用2D扩散模型的光照先验进行自监督优化的框架，以改进单目深度估计在跨域场景下的性能。"
  },
  {
    "title": "Dexterous World Models",
    "url": "http://arxiv.org/abs/2512.17907v1",
    "abstract_en": "Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.   Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.   Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.",
    "date": "2025-12-19",
    "summary_cn": "当前3D数字孪生多为静态，缺乏交互性。为解决此问题，本文提出Dexterous World Model (DWM)，这是一种基于场景和动作条件的视频扩散模型。它通过输入静态3D场景渲染图和以自我为中心的手部运动序列，生成时空一致、展现人手与场景进行如抓取、移动物体等逼真交互的视频。该方法利用合成数据与真实世界视频相结合的数据集进行训练，在保持场景一致性的同时，实现了物理合理的动态交互模拟，为构建交互式数字孪生迈出了重要一步。",
    "one_sentence": "本文提出了Dexterous World Model (DWM)，一种结合静态3D场景和手部动作的视频扩散框架，用于生成具有交互性的动态数字孪生。"
  },
  {
    "title": "Fisher information for the multi-species Landau system",
    "url": "http://arxiv.org/abs/2512.17905v1",
    "abstract_en": "We consider the Fisher information for spatially homogeneous multi-species Landau system. We show that the mass-weighted Fisher information is monotone decreasing in time along the solutions of the Landau system with a general class of interaction potentials.",
    "date": "2025-12-19",
    "summary_cn": "本研究探讨了空间齐次多物种Landau系统中的Fisher信息。作者证明了，在相当广泛的一类相互作用势条件下，沿着Landau系统解的时间演化，其质量加权的Fisher信息是单调递减的。这一结果为该系统在时间方向上的信息量变化提供了重要的数学描述。",
    "one_sentence": "本文证明了对于一大类相互作用势，多物种Landau方程解的质量加权Fisher信息随时间单调递减。"
  },
  {
    "title": "Plane Strong Connectivity Augmentation",
    "url": "http://arxiv.org/abs/2512.17904v1",
    "abstract_en": "We investigate the problem of strong connectivity augmentation within plane oriented graphs.   We show that deciding whether a plane oriented graph $D$ can be augmented with (any number of) arcs $X$ such that $D+X$ is strongly connected, but still plane and oriented, is NP-hard.   This question becomes trivial within plane digraphs, like most connectivity augmentation problems without a budget constraint.   The budgeted version, Plane Strong Connectivity Augmentation (PSCA) considers a plane oriented graph $D$ along with some integer $k$, and asks for an $X$ of size at most $k$ ensuring that $D+X$ is strongly connected, while remaining plane and oriented.   Our main result is a fixed-parameter tractable algorithm for PSCA, running in time $2^{O(k)} n^{O(1)}$.   The cornerstone of our procedure is a structural result showing that, for any fixed $k$, each face admits a bounded number of partial solutions \"dominating\" all others.   Then, our algorithm for PSCA combines face-wise branching with a Monte-Carlo reduction to the polynomial Minimum Dijoin problem, which we derandomize.   To the best of our knowledge, this is the first FPT algorithm for a (hard) connectivity augmentation problem constrained by planarity.",
    "date": "2025-12-19",
    "summary_cn": "该研究探讨了平面有向图的强连通性增强问题。作者证明了，在不限制新增边数量的情况下，判定一个平面有向图能否通过加边变得强连通且保持平面性是一个NP难问题。而对于有预算限制的PSCA问题（即最多添加k条边），他们提出了一个固定参数可解（FPT）算法，其核心是证明每个面仅需考虑有限个“支配性”部分解，并结合了确定化的蒙特卡洛归约技术。据作者所知，这是首个针对受平面性约束的（困难）连通性增强问题的FPT算法。",
    "one_sentence": "本文针对平面有向图的强连通性增强问题，首次提出了一个运行时间为2^{O(k)} n^{O(1)}的固定参数可解（FPT）算法。"
  }
]