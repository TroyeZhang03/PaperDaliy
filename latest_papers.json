[
  {
    "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
    "url": "http://arxiv.org/abs/2602.06043v1",
    "abstract_en": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.",
    "date": "2026-02-05",
    "summary_cn": "现有参数高效微调方法（如LoRA）在持续学习时存在灾难性遗忘问题，且需要存储多个适配器。本文提出Share方法，通过构建并动态更新一个共享的低秩子空间，从历史任务中提取核心知识并增量整合新信息。该方法在减少参数与内存消耗的同时，保持了与联合训练模型相当的性能，支持大规模AI系统的可扩展持续学习。在多个视觉和语言任务上的实验验证了其有效性。",
    "one_sentence": "本文提出了一种名为Share的参数高效持续微调方法，通过学习和动态更新一个共享的低秩子空间，实现跨任务和跨模态的无缝适应，同时显著降低存储开销。"
  },
  {
    "title": "Pseudo-Invertible Neural Networks",
    "url": "http://arxiv.org/abs/2602.06042v1",
    "abstract_en": "The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or \"Back-Projection\", $x' = x + A^\\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, \"degradation\" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.",
    "date": "2026-02-05",
    "summary_cn": "该论文提出了一种将Moore-Penrose伪逆推广至非线性映射（尤其是神经网络）的方法，并设计了具有可处理非线性伪逆的Surjective Pseudo-invertible Neural Networks。关键贡献是定义了非线性逆投影方法，保证非线性映射满足一致性约束。这扩展了零样本求解线性反问题的技术，使其能应用于光学畸变、分类等非线性信息退化场景，从而实现对复杂退化过程的零样本反演和对生成输出的精确语义控制。",
    "one_sentence": "本文提出了一种称为Surjective Pseudo-invertible Neural Networks的架构，将Moore-Penrose伪逆推广到非线性映射，并基于此定义了非线性逆投影方法，扩展了零样本反问题的求解范围。"
  },
  {
    "title": "Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning",
    "url": "http://arxiv.org/abs/2602.06041v1",
    "abstract_en": "Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.",
    "date": "2026-02-05",
    "summary_cn": "当前多模态大语言模型在多图像空间推理方面存在挑战。本文针对从多视角观察中建立连贯3D理解并进行新视点推理的问题，提出了CAMCUE这一相机姿态感知的多图像框架。该框架通过向视觉标记注入各视角的相机姿态、将语言描述的对齐目标姿态，并合成条件化想象的视觉目标来辅助回答问题。为此，作者构建了包含大量多视图图像、姿态及视角描述问答的数据集CAMCUE-DATA。实验表明，CAMCUE显著提升了推理准确率，并能高效地将语言描述映射为精确的相机姿态，极大加快了推理速度。",
    "one_sentence": "本文提出了CAMCUE框架，通过显式利用相机姿态作为几何锚点来实现多视角图像的空间推理和基于语言描述的新视点推理。"
  },
  {
    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
    "url": "http://arxiv.org/abs/2602.06039v1",
    "abstract_en": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.",
    "date": "2026-02-05",
    "summary_cn": "现有的提示大语言模型构建的多智能体系统通常采用固定的通信模式，难以适应迭代问题解决不同阶段的需求。为此，本文提出了DyTopo框架，其通过一个管理器在每轮设定目标，各智能体输出需求与提供的关键词描述符，框架据此进行语义匹配并动态构建稀疏有向通信图，仅允许信息沿匹配的边路由。在代码生成和数学推理基准测试中，DyTopo在多种大模型上均显著超越了最强基线，并且动态通信图提供了可解释的协调轨迹。",
    "one_sentence": "本文提出了一种名为DyTopo的动态拓扑多智能体框架，其在每一轮推理中根据需求语义匹配重建稀疏通信图，以优化迭代问题解决中的协作效率。"
  },
  {
    "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
    "url": "http://arxiv.org/abs/2602.06040v1",
    "abstract_en": "Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as \"visual thoughts\" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.",
    "date": "2026-02-05",
    "summary_cn": "现有多模态大模型通常依赖文本思维链，难以有效处理视觉密集任务，而引入视觉模态又会影响文本逻辑推理。为解决此问题，本研究提出SwimBird模型，它采用混合自回归框架，能根据输入内容自适应地在纯文本、纯视觉及视觉-文本交织三种推理模式间动态切换。通过构建专门的数据集并进行训练，该模型在保持强大文本推理能力的同时，显著提升了视觉密集任务的性能，在多项基准测试中取得了领先结果。",
    "one_sentence": "本文提出了一种名为SwimBird的推理可切换多模态大语言模型，能根据输入动态选择纯文本、纯视觉或视觉-文本交织的推理模式。"
  }
]