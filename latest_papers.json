[
  {
    "title": "Evaluation of Oncotimia: An LLM based system for supporting tumour boards",
    "url": "http://arxiv.org/abs/2601.19899v1",
    "abstract_en": "Multidisciplinary tumour boards (MDTBs) play a central role in oncology decision-making but require manual processes and structuring large volumes of heterogeneous clinical information, resulting in a substantial documentation burden. In this work, we present ONCOTIMIA, a modular and secure clinical tool designed to integrate generative artificial intelligence (GenAI) into oncology workflows and evaluate its application to the automatic completion of lung cancer tumour board forms using large language models (LLMs). The system combines a multi-layer data lake, hybrid relational and vector storage, retrieval-augmented generation (RAG) and a rule-driven adaptive form model to transform unstructured clinical documentation into structured and standardised tumour board records. We assess the performance of six LLMs deployed through AWS Bedrock on ten lung cancer cases, measuring both completion form accuracy and end-to-end latency. The results demonstrate high performance across models, with the best performing configuration achieving an 80% of correct field completion and clinically acceptable response time for most LLMs. Larger and more recent models exhibit best accuracies without incurring prohibitive latency. These findings provide empirical evidence that LLM- assisted autocompletion form is technically feasible and operationally viable in multidisciplinary lung cancer workflows and support its potential to significantly reduce documentation burden while preserving data quality.",
    "date": "2026-01-27",
    "summary_cn": "为了解决肿瘤多学科会诊中信息处理繁重、文档负担大的问题，本研究提出了ONCOTIMIA工具。该工具整合了生成式人工智能，利用多源数据架构和检索增强生成等技术，能够自动处理非结构化临床数据并生成标准化的会诊表单。在针对十个肺癌病例的评估中，最优模型配置达到了80%的正确字段填充率，且响应时间符合临床要求。结果表明，基于大语言模型的表单自动填充在技术上可行，有潜力在保证数据质量的同时显著减轻文书工作负担。",
    "one_sentence": "本文提出了一个名为ONCOTIMIA的模块化临床工具，通过集成生成式AI和检索增强生成等技术，能够将非结构化的临床文档自动转换为结构化的肺癌多学科会诊记录，以减轻文档负担。"
  },
  {
    "title": "DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding",
    "url": "http://arxiv.org/abs/2601.19898v1",
    "abstract_en": "Arabic calligraphy represents one of the richest visual traditions of the Arabic language, blending linguistic meaning with artistic form. Although multimodal models have advanced across languages, their ability to process Arabic script, especially in artistic and stylized calligraphic forms, remains largely unexplored. To address this gap, we present DuwatBench, a benchmark of 1,272 curated samples containing about 1,475 unique words across six classical and modern calligraphic styles, each paired with sentence-level detection annotations. The dataset reflects real-world challenges in Arabic writing, such as complex stroke patterns, dense ligatures, and stylistic variations that often challenge standard text recognition systems. Using DuwatBench, we evaluated 13 leading Arabic and multilingual multimodal models and showed that while they perform well on clean text, they struggle with calligraphic variation, artistic distortions, and precise visual-text alignment. By publicly releasing DuwatBench and its annotations, we aim to advance culturally grounded multimodal research, foster fair inclusion of the Arabic language and visual heritage in AI systems, and support continued progress in this area. Our dataset (https://huggingface.co/datasets/MBZUAI/DuwatBench) and evaluation suit (https://github.com/mbzuai-oryx/DuwatBench) are publicly available.",
    "date": "2026-01-27",
    "summary_cn": "本文指出多模态模型在处理艺术化的阿拉伯语书法方面存在不足，为此构建了名为 DuwatBench 的新基准数据集。该数据集包含超过1200个涵盖六种古典与现代书法风格的样本，并配有句子级检测标注，真实反映了复杂笔画、连字与风格变化等挑战。研究使用该基准评估了13个领先模型，发现它们在常规文本上表现良好，但在书法变体、艺术扭曲及图文对齐方面存在显著困难。通过公开发布数据集与评测工具，旨在推动包含阿拉伯语言与视觉遗产在内的、更具文化根基的多模态研究。",
    "one_sentence": "本文提出了首个专门针对多种阿拉伯语书法风格、包含精细标注的评测基准 DuwatBench，以评估和推动多模态模型对艺术化文本的理解能力。"
  },
  {
    "title": "Self-Distillation Enables Continual Learning",
    "url": "http://arxiv.org/abs/2601.19897v1",
    "abstract_en": "Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.",
    "date": "2026-01-27",
    "summary_cn": "持续学习对于基础模型至关重要，但传统的监督微调存在遗忘问题，而在线强化学习又常缺乏明确的奖励函数。为此，论文提出了自蒸馏微调方法，该方法利用演示条件模型自身的上下文学习能力来生成在线策略的训练信号，使其既是学习者又是教师。实验表明，该方法在技能学习和知识获取任务上均能有效提高新任务性能，同时显著降低灾难性遗忘，证明了基于演示的在线策略蒸馏是实现持续学习的可行路径。",
    "one_sentence": "本文提出了一种名为SDFT的自蒸馏微调方法，通过使模型利用自身作为教师来生成在线策略的训练信号，从而在模仿专家演示时实现持续学习并减少遗忘。"
  },
  {
    "title": "Real-Time Iteration Scheme for Dynamical Mean-Field Theory: A Framework for Near-Term Quantum Simulation",
    "url": "http://arxiv.org/abs/2601.19896v1",
    "abstract_en": "We present a time-domain iteration scheme for solving the Dynamical Mean-Field Theory (DMFT) self-consistent equations using retarded Green's functions in real time. Unlike conventional DMFT approaches that operate in imaginary time or frequency space, our scheme operates directly with real-time quantities. This makes it particularly suitable for near-term quantum computing hardware with limited Hilbert spaces, where real-time propagation can be efficiently implemented via Trotterization or variational quantum algorithms. We map the effective impurity problem to a finite one-dimensional chain with a small number of bath sites, solved via exact diagonalization as a proof-of-concept. The hybridization function is iteratively updated through time-domain fitting until self-consistency. We demonstrate stable convergence across a wide range of interaction strengths for the half-filled Hubbard model on a Bethe lattice, successfully capturing the metal-to-insulator transition. Despite using limited time resolution and a minimal bath discretization, the spectral functions clearly exhibit the emergence of Hubbard bands and the suppression of spectral weight at the Fermi level as interaction strength increases. This overcomes major limitations of two-site DMFT approximations by delivering detailed spectral features while preserving efficiency and compatibility with quantum computing platforms through real-time dynamics.",
    "date": "2026-01-27",
    "summary_cn": "本文提出了一种在实时间域求解动力学平均场理论自洽方程的迭代方案，该方案直接操作实时间量，适用于可实现实时传播的近期量子计算硬件。作者将有效杂质问题映射到有限的一维链，通过精确对角化在时间域拟合更新杂化函数直至自洽，成功演示了半填充贝特晶格上哈伯德模型的稳定收敛及金属-绝缘体相变。该方法克服了传统两点近似的局限，在保证效率的同时展现了详细的光谱特征。",
    "one_sentence": "本文提出了一种直接在实时间域求解动力学平均场理论的迭代方案，该方法特别适用于希尔伯特空间有限的近期量子计算硬件。"
  },
  {
    "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep",
    "url": "http://arxiv.org/abs/2601.19895v1",
    "abstract_en": "Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.",
    "date": "2026-01-27",
    "summary_cn": "针对现有Transformer架构在深度扩展时训练不稳定的问题，本研究揭示了Post-LayerNorm失败的关键原因是残差连接导致的梯度消失。为此，论文提出了Keel模型，用高速网络（Highway）式连接取代传统残差路径，从而保护了梯度流动。该方法无需特殊初始化或复杂优化技巧，即可稳定训练超过1000层的深度模型，在困惑度和深度扩展性上均优于主流的Pre-LN架构，为构建无限深度的大语言模型提供了新基础。",
    "one_sentence": "本文提出了Keel模型，通过将Post-LayerNorm Transformer中的残差连接替换为高速网络式连接，解决了深度训练时的梯度消失问题，实现了超深（超过1000层）大语言模型的稳定训练。"
  }
]