[
  {
    "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
    "url": "http://arxiv.org/abs/2512.07834v1",
    "abstract_en": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/",
    "date": "2025-12-08",
    "summary_cn": "Voxel艺术在游戏和数字媒体中应用广泛，但现有方法难以在几何抽象、语义保持和离散色彩一致性之间取得平衡。本文提出Voxify3D框架，其核心创新在于融合了三个组件：利用正交投影消除透视变形以实现精确体素-像素对齐；通过基于补丁的CLIP模型保持不同离散化程度下的语义；采用调色板约束的Gumbel-Softmax量化实现可微分的离散色彩空间优化。实验表明，该方法在多种角色模型和可控抽象级别上均表现出色。",
    "one_sentence": "本文提出了Voxify3D，一个通过正交像素艺术监督、基于补丁的语义对齐和可微分调色板量化，将3D网格优化与2D像素艺术指导相结合的可微分两阶段框架。"
  },
  {
    "title": "Relational Visual Similarity",
    "url": "http://arxiv.org/abs/2512.07833v1",
    "abstract_en": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
    "date": "2025-12-08",
    "summary_cn": "人类能够感知超越表面属性的关系相似性，但现有视觉相似性度量标准（如LPIPS、CLIP）均无法捕捉这种能力。为填补这一关键空白，本文首先将关系相似性定义为可测量问题，即图像内部视觉元素间的关系或功能相对应。研究者进而构建了一个包含11.4万条图像-匿名描述的数据集，描述内容聚焦于场景的底层关系逻辑而非表面内容。利用该数据集微调视觉语言模型后，得到了首个能够衡量图像间关系相似性的模型，为基于关系结构而非外观连接图像迈出了第一步。",
    "one_sentence": "本文通过构建一个描述图像内部关系逻辑的数据集，并微调视觉语言模型，首次实现了对图像间关系相似性的度量。"
  },
  {
    "title": "Do Generalisation Results Generalise?",
    "url": "http://arxiv.org/abs/2512.07832v1",
    "abstract_en": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.",
    "date": "2025-12-08",
    "summary_cn": "传统评估大语言模型分布外泛化能力的方法通常只使用单一数据集，可能无法准确反映模型面对多样数据变化时的真实表现。本研究创新性地通过分析模型在微调过程中于多个分布外测试集上的性能，并控制其域内性能，来探究不同泛化结果之间的相关性。研究发现，对于OLMo2和OPT模型，泛化性能之间不存在统一的关联模式，其正负相关性完全取决于所分析的具体模型。",
    "one_sentence": "本文通过评估模型在多个分布外测试集上的表现并控制域内性能，揭示了模型泛化能力之间不存在普遍相关性，其结果高度依赖于具体模型。"
  },
  {
    "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
    "url": "http://arxiv.org/abs/2512.07831v1",
    "abstract_en": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo",
    "date": "2025-12-08",
    "summary_cn": "针对现有视频生成模型因单模态条件限制而缺乏整体世界理解的问题，本研究提出了UnityVideo框架。该框架通过联合学习多种模态（如分割掩码、人体骨架等）和训练范式，并引入了动态加噪与带有上下文学习器的模态切换器两大核心技术。研究还构建了包含130万样本的大规模数据集。实验表明，UnityVideo能加速收敛，显著提升零样本泛化能力，并生成质量更高、更符合物理约束的视频。",
    "one_sentence": "本文提出了一个名为UnityVideo的统一视频生成框架，通过动态加噪和模态切换器等创新技术，实现了多模态联合学习以增强对物理世界的理解和生成能力。"
  },
  {
    "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
    "url": "http://arxiv.org/abs/2512.07829v1",
    "abstract_en": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.",
    "date": "2025-12-08",
    "summary_cn": "针对预训练视觉表征与生成模型潜空间不匹配的挑战，本文提出了特征自编码器（FAE）框架。该框架通过一个简单的注意力层，将高维理解性特征转换为低维生成性潜变量。其核心创新是耦合两个解码器：一个用于特征重建，另一个用于图像生成。FAE具有通用性，可与多种自监督编码器和生成模型结合，在多项基准测试中取得了接近或达到最先进的性能，兼顾了高质量与快速学习。",
    "one_sentence": "本文提出了一种名为FAE的简单框架，通过耦合两个深度解码器，将预训练的视觉表征有效适配到适用于生成任务的低维潜空间中。"
  }
]