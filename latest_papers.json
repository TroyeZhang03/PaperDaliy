[
  {
    "title": "LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context",
    "url": "http://arxiv.org/abs/2511.19437v1",
    "abstract_en": "Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.",
    "date": "2025-11-24",
    "summary_cn": "针对现有基于物理的渲染纹理生成方法在材质分解和纹理补全上的不足，本文提出LumiTex框架。该框架包含三个核心组件：利用共享光照先验解耦反照率与金属粗糙度的多分支生成方案、将光照信息注入解码过程的光照感知注意力机制，以及基于大视图合成模型确保无缝且视角一致纹理补全的几何引导修复模块。大量实验证明，LumiTex在纹理质量上超越了现有开源和商业方法，达到了最先进的性能。",
    "one_sentence": "本文提出了名为LumiTex的端到端框架，通过多分支生成、光照感知注意力机制和几何引导修复，解决了从图像生成高质量PBR纹理时的材质分解和纹理补全难题。"
  },
  {
    "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
    "url": "http://arxiv.org/abs/2511.19436v1",
    "abstract_en": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.",
    "date": "2025-11-24",
    "summary_cn": "该论文介绍了VDC-Agent，一个用于视频详细描述的自进化框架。该框架无需人工标注或大型教师模型，通过字幕生成、原则性评分和提示优化的闭环流程自动运行。当质量下降时，系统会启动自我反思路径进行修正。作者利用该流程在未标记视频上生成数据轨迹，并转换为包含18,886对数据的VDC-Agent-19K数据集。基于Qwen2.5-VL-7B模型进行课程直接偏好优化微调后，VDC-Agent-7B在VDC基准测试中以49.08%的平均准确率实现最先进性能，相比基础模型有显著提升。",
    "one_sentence": "本文提出了一种无需人工标注或大教师模型的自我进化视频描述框架VDC-Agent，通过生成、评分、反思的闭环流程自动构建高质量训练数据，并在基准测试中取得领先性能。"
  },
  {
    "title": "Are Image-to-Video Models Good Zero-Shot Image Editors?",
    "url": "http://arxiv.org/abs/2511.19435v1",
    "abstract_en": "Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.",
    "date": "2025-11-24",
    "summary_cn": "本文提出IF-Edit框架，旨在利用预训练的大规模视频扩散模型进行零样本图像编辑。该框架解决了提示错位、冗余时序潜在表示和后期帧模糊三大挑战，具体包括一个思维链提示增强模块、一个时序潜在表示丢弃策略和一个自一致的后处理精炼步骤。实验结果表明，IF-Edit在需要推理的任务上表现优异，同时在通用编辑任务上保持竞争力，为统一视频-图像生成推理提供了一种简单方案。",
    "one_sentence": "本文提出了一个名为IF-Edit的免调优框架，通过提示增强、时序潜在表示丢弃和后处理精炼三项技术，将预训练的视频扩散模型重新用于指令驱动的图像编辑。"
  },
  {
    "title": "Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts",
    "url": "http://arxiv.org/abs/2511.19434v1",
    "abstract_en": "Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.",
    "date": "2025-11-24",
    "summary_cn": "扩散模型在图像生成中存在样本感知质量与数据似然之间的权衡。为解决此问题，本文提出一种即插即用的采样方法，在去噪过程中结合两个预训练专家模型：首先使用擅长图像质量的专家处理高噪声以构建全局结构，然后切换至擅长似然的专家处理低噪声以优化细节。该方法无需重新训练，仅在CIFAR-10和ImageNet32上实验表明，其能同时提升或保持似然指标与视觉质量，有效打破了原有权衡。",
    "one_sentence": "本文提出了一种在去噪轨迹上切换使用两个预训练扩散专家模型的新采样方法，以突破图像生成中感知质量与数据似然之间的权衡。"
  },
  {
    "title": "Mixture of Horizons in Action Chunking",
    "url": "http://arxiv.org/abs/2511.19433v1",
    "abstract_en": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
    "date": "2025-11-24",
    "summary_cn": "视觉-语言-动作（VLA）模型在机器人操作中表现出色，但其性能受限于训练时固定的动作序列长度（视野）。本文研究发现长视野与短视野之间存在权衡关系。为克服此局限，作者提出了混合视野（MoH）策略，它将动作块分割为不同视野的段落并行处理，并通过线性门融合结果。该方法能在一个模型内兼顾长期预见性和短期精确性，且易于集成、开销小，还能实现动态自适应推理。实验表明，MoH在仿真和真实任务中均带来显著性能提升，并在混合任务设置下达到了新的最高水平。",
    "one_sentence": "本文提出了一种混合预测视野（MoH）策略，通过在单一模型中并行处理不同长度的动作序列并融合输出，以同时获得长期规划能力和短期控制精度。"
  }
]