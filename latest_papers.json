[
  {
    "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
    "url": "http://arxiv.org/abs/2601.16214v1",
    "abstract_en": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.",
    "date": "2026-01-22",
    "summary_cn": "针对现有相机可控视频扩散模型在相机对齐能力和计算效率上的不足，本文提出了一种新方法。核心是引入一个高效的相机感知3D解码器，它将视频潜在表征与相机位姿解码为3D高斯表示。通过分析因对齐不良导致的几何畸变和模糊渲染，方法将渲染新视图与真实视图的像素一致性优化为奖励信号，并引入可见性项来处理随机性。在RealEstate10K和WorldScore基准测试上的实验验证了该方法的有效性。",
    "one_sentence": "本文提出了一种高效的相机感知3D解码器，通过将视频潜在表征解码为3D高斯表示并利用渲染视图一致性来量化奖励，从而显著提升了视频扩散模型的相机控制能力。"
  },
  {
    "title": "Gauge Theory and Skein Modules",
    "url": "http://arxiv.org/abs/2601.16213v1",
    "abstract_en": "We study skein modules of 3-manifolds by embedding them into the Hilbert spaces of 4d ${\\cal N}=4$ super-Yang-Mills theories. When the 3-manifold has reduced holonomy, we present an algorithm to determine the dimension and the list of generators of the skein module with a general gauge group. The analysis uses a deformation preserving ${\\cal N}=1$ supersymmetry to express the dimension as a sum over nilpotent orbits. We find that the dimensions often differ between Langlands-dual pairs beyond the A-series, for which we provide a physical explanation involving chiral symmetry breaking and 't Hooft operators. We also relate our results to the structure of $\\mathbb{C}^*$-fixed loci in the moduli space of Higgs bundles. This approach helps to clarify the relation between the gauge-theoretic framework of Kapustin and Witten with other versions of the geometric Langlands program, explains why the dimensions of skein modules do not exhibit a TQFT-like behavior, and provides a physical interpretation of the skein-valued curve counting of Ekholm and Shende.",
    "date": "2026-01-22",
    "summary_cn": "本研究将三维流形的skein模嵌入到四维N=4超对称杨-米尔斯理论的希尔伯特空间中进行分析。对于具有简化完整群的流形，作者提出了一种算法，通过保留N=1超对称性的形变，将skein模的维度表达为零化轨道的和，从而确定其维度和生成元。研究发现，朗兰兹对偶对的skein模维度在A系列之外常不相同，并利用手征对称性破缺和‘t Hooft算符给出了物理解释。该工作联系了希格斯丛模空间的固定点，有助于阐明卡普斯汀和维滕的规范理论与几何朗兰兹纲领其他版本的关系，并解释了skein模维度不呈现拓扑量子场论行为的原因。",
    "one_sentence": "本文通过将三维流形的skein模嵌入四维超对称杨-米尔斯理论的希尔伯特空间，提出了一种计算其维数和生成元的算法，并用物理原理解释了朗兰兹对偶下的维数差异。"
  },
  {
    "title": "Point Bridge: 3D Representations for Cross Domain Policy Learning",
    "url": "http://arxiv.org/abs/2601.16212v1",
    "abstract_en": "Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/",
    "date": "2026-01-22",
    "summary_cn": "当前通用机器人模型的发展受限于真实世界数据的匮乏。本文提出的Point Bridge框架通过使用视觉语言模型提取统一的点基表示，结合基于Transformer的策略学习，能够仅使用合成数据进行训练，并实现高效的零样本仿真到现实策略迁移。该方法无需显式的视觉或对象级对齐，仅需少量真实演示数据协同训练即可进一步提升性能，在单任务和多任务场景下均大幅超越现有方法。",
    "one_sentence": "本文提出了Point Bridge框架，利用领域无关的点表示，实现了无需视觉对齐的、零样本的从仿真到现实的策略迁移。"
  },
  {
    "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition",
    "url": "http://arxiv.org/abs/2601.16211v1",
    "abstract_en": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.",
    "date": "2026-01-22",
    "summary_cn": "本文研究了组合视频理解（CVU）中的零样本组合动作识别（ZS-CAR）问题。现有模型因存在物体驱动的动词捷径而表现不佳，这源于监督数据的稀疏、偏斜以及动词与对象学习难度的不对称。为此，作者提出了RCORE框架，该框架包含（i）组合感知增强，在不破坏运动线索的情况下多样化动词-对象组合，以及（ii）时序顺序正则化损失，通过显式建模时序结构来惩罚捷径行为。在两个基准数据集上的实验表明，RCORE显著提升了未见组合的识别准确率，降低了对共现统计的依赖，并实现了持续为正的组合泛化优势。",
    "one_sentence": "本文提出了一个名为RCORE的框架，通过组合感知增强和时序顺序正则化损失来强制进行时序扎根的动词学习，以解决零样本组合动作识别中存在的物体驱动的动词捷径问题。"
  },
  {
    "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
    "url": "http://arxiv.org/abs/2601.16210v1",
    "abstract_en": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
    "date": "2026-01-22",
    "summary_cn": "针对现有离散视频变分自编码器（VAE）在跨模态对齐和零样本迁移方面的不足，本文提出了PyraTok方法。PyraTok是一种语言对齐的金字塔型标记化模型，它在预训练视频VAE基础上，利用新设计的语言对齐金字塔量化（LaPQ）模块，在多个时空分辨率下共享大型二进制码本进行离散化，生成紧凑而富有表现力的视频标记序列。通过联合优化多尺度文本引导量化和对标记层级的全局自回归目标，该方法紧密耦合了视觉与语言信息。实验表明，PyraTok在视频重建、文本生成视频质量上表现优异，并在视频分割、时序动作定位和理解等任务的零样本性能上达到了新的先进水平，能够稳健扩展到4K/8K分辨率。",
    "one_sentence": "本文提出了一种名为PyraTok的语言对齐金字塔型视频标记化方法，通过共享大型二进制码本和联合优化多尺度量化与自回归目标，显著提升了视频生成与理解任务的效果。"
  }
]