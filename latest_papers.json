[
  {
    "title": "EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI",
    "url": "http://arxiv.org/abs/2512.02020v1",
    "abstract_en": "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.",
    "date": "2025-12-01",
    "summary_cn": "生成模型在具身AI的策略学习中面临数据利用率和采样效率低下的挑战。本文提出EfficientFlow框架，通过引入流匹配的等变性理论保证来提升数据效率，并设计了一种新颖的加速正则化策略以加快动作生成。在多个机器人操作任务上的实验表明，该方法在数据受限条件下性能优越，且推理速度显著提升。",
    "one_sentence": "本文提出了一种名为EfficientFlow的、基于流模型的高效具身AI策略学习框架，通过引入等变性和加速正则化策略，同时解决了数据利用率和动作采样效率低下的问题。"
  },
  {
    "title": "A Diffusion Model Framework for Maximum Entropy Reinforcement Learning",
    "url": "http://arxiv.org/abs/2512.02019v1",
    "abstract_en": "Diffusion models have achieved remarkable success in data-driven learning and in sampling from complex, unnormalized target distributions. Building on this progress, we reinterpret Maximum Entropy Reinforcement Learning (MaxEntRL) as a diffusion model-based sampling problem. We tackle this problem by minimizing the reverse Kullback-Leibler (KL) divergence between the diffusion policy and the optimal policy distribution using a tractable upper bound. By applying the policy gradient theorem to this objective, we derive a modified surrogate objective for MaxEntRL that incorporates diffusion dynamics in a principled way. This leads to simple diffusion-based variants of Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO) and Wasserstein Policy Optimization (WPO), termed DiffSAC, DiffPPO and DiffWPO. All of these methods require only minor implementation changes to their base algorithm. We find that on standard continuous control benchmarks, DiffSAC, DiffPPO and DiffWPO achieve better returns and higher sample efficiency than SAC and PPO.",
    "date": "2025-12-01",
    "summary_cn": "该研究将最大熵强化学习（MaxEntRL）重新定义为扩散模型的采样问题，通过最小化扩散策略与最优策略分布间的反向KL散度，并利用策略梯度定理推导出新的代理目标。基于此，作者提出了DiffSAC、DiffPPO和DiffWPO等扩散变体算法。实验表明，这些方法仅需对原算法进行微小改动，即可在连续控制任务中获得更高的回报和样本效率。",
    "one_sentence": "本文提出将最大熵强化学习重新解释为基于扩散模型的采样问题，并推导出新的代理目标函数，从而开发出性能更优的DiffSAC、DiffPPO和DiffWPO算法。"
  },
  {
    "title": "Data-Centric Visual Development for Self-Driving Labs",
    "url": "http://arxiv.org/abs/2512.02018v1",
    "abstract_en": "Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.",
    "date": "2025-12-01",
    "summary_cn": "自驱实验室（SDL）的精准操作如移液，需要大量标注数据来训练鲁棒模型，但负样本等数据难以获取。为此，本研究构建了一个混合流水线：真实数据流采用人在环策略，结合自动采集与人工验证以保证精度；虚拟数据流则通过参考条件提示的图像生成技术来增强数据，并经筛选确保可靠性。该方法生成了类别均衡的数据集，使气泡检测模型在真实测试集上达到99.6%的准确率，混合训练也能维持99.4%的精度，同时大幅降低数据收集与审核成本，为SDL的视觉反馈提供了可扩展且经济高效的解决方案。",
    "one_sentence": "本文提出了一种融合真实数据采集与虚拟数据生成的人机协同混合流水线，以解决自驱实验室中因训练数据稀缺（尤其是负样本）导致的模型鲁棒性问题。"
  },
  {
    "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
    "url": "http://arxiv.org/abs/2512.02017v1",
    "abstract_en": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
    "date": "2025-12-01",
    "summary_cn": "针对多台消费级相机拍摄的视频流难以同步的问题，本文提出了VisualSync框架。该方法的创新在于利用移动的3D点在共视相机间满足极几何约束这一原理，通过结合现成的三维重建、特征匹配和密集跟踪技术，联合最小化极线误差来估计每个相机的时间偏移量。在四个不同且具有挑战性的数据集上的实验表明，VisualSync优于现有基线方法，其中位同步误差低于50毫秒。",
    "one_sentence": "本文提出了一个名为VisualSync的优化框架，通过利用多视角动态和极几何约束，实现了对非固定、未同步视频的高精度毫秒级自动对齐。"
  },
  {
    "title": "Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now",
    "url": "http://arxiv.org/abs/2512.02016v1",
    "abstract_en": "Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\\mathrm{eff}}$ from $1.81\\,\\mathrm{m/s^2}$ to $6.43\\,\\mathrm{m/s^2}$ (reaching $65\\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.",
    "date": "2025-12-01",
    "summary_cn": "本研究探讨了视频生成模型作为世界模型时对重力定律的表示能力。研究发现，现有模型生成的下落物体加速度普遍偏慢。为排除帧率等度量尺度干扰，作者设计了一种无单位的两物体测试协议，发现模型违反了伽利略等效原理。通过仅使用100个单球下落视频对模型进行轻量级微调，可将有效重力加速度从1.81 m/s²提升至6.43 m/s²（达到地球重力的65%），且该修正能零样本泛化到更复杂的物理场景。",
    "one_sentence": "本文提出了一种不受度量尺度干扰的、基于两物体下落时序比的无单位测试协议，揭示了视频生成模型违反伽利略等效原理的物理缺陷，并通过少量数据微调成功部分修正了该缺陷。"
  }
]